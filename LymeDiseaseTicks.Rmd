---
title: "Lyme Disease Ticks"
author: "Janis Corona"
date: "8/26/2020"
output: html_document
---

This data analysis is on lyme disease using GEO series data made readily available in its normalized state from [GSE145974](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE145974) on ncbi.nlm.nih.gov as the accession number. The data is from the [platform GPL13667](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL13667) and the [series data](https://ftp.ncbi.nlm.nih.gov/geo/series/GSE145nnn/GSE145974/matrix/). There are also some CEL/TAR files for Ubuntu but I couldn't get my ubuntu machines to recognize it, and the instructions and tutorials for accessing the SRAtoolkit and using the Windows Ubuntu app, didn't avail, so I am using the text files only.If you have a windows 10 tutorial on running SRAtoolkit using the ubuntu app for windows or getting the cel files to work on ubuntu with a VirtualBox disk image of ubuntu that works because you tried it within 24 hours and it worked exactly as explained, please share. I have yet to get those up and running. Possibly the new updates to virtualBox or my other apps, like docker or MongoDB or Tableau are interfering. I am not going to waste time figuring it out, its a time trap.  

All the data was there as far as being filled out with values for the feature names, because I do recall exploring some platforms and series downloadable text files and only the header information was there and none of the values. The method used for processing was expression profiling by microarray on peripheral blood mononucleated cells (PBMC). The values seem to be scaled or normalized already as the values are inclusive of negative values. 

***
There is a research article available from the researchers on this study to accompany the data for free through [pubMed](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7078463/pdf/mBio.00047-20.pdf):

"Global Transcriptome Analysis Identifies a Diagnostic
Signature for Early Disseminated Lyme Disease and Its Resolution" authored by the following researchers:
Mary M. Petzke,a Konstantin Volyanskyy,b Yong Mao,b Byron Arevalo,a Raphael Zohn,a Johanna Quituisaca,a Gary P. Wormser,c Nevenka Dimitrova,b Ira Schwartza

They are with the Department of Microbiology and Immunology, School of Medicine, New York Medical College, Valhalla, New York, USA bPhillips Research North America, Valhalla, New York, USA
Division of Infectious Diseases, Department of Medicine, New York Medical College, Valhalla, New York, USA
**Citation** Petzke MM, Volyanskyy K, Mao Y,
Arevalo B, Zohn R, Quituisaca J, Wormser GP, Dimitrova N, Schwartz I. 2020. Global transcriptome analysis identifies a diagnostic signature for early disseminated Lyme disease and its resolution. mBio 11:e00047-20. https:// doi.org/10.1128/mBio.00047-20.
Editor Steven J. Norris, McGovern Medical
School
Copyright Â© 2020 Petzke et al. This is an openaccess article distributed under the terms of the Creative Commons Attribution 4.0 International license.
Address correspondence to Mary M. Petzke, mpetzke@nymc.edu.
This article is a direct contribution from Ira Schwartz, a Fellow of the American Academy of Microbiology, who arranged for and secured reviews by Patricia Rosa, NIAID, NIH, and John Leong, Tufts University School of Medicine.
Received 9 January 2020
Accepted 31 January 2020 **Published 17 March 2020**
"**ABSTRACT** A bioinformatics approach was employed to identify transcriptome alterations in the peripheral blood mononuclear cells of well-characterized human subjects who were diagnosed with early disseminated Lyme disease (LD) based on stringent microbiological and clinical criteria. Transcriptomes were assessed at the time of presentation and also at approximately 1 month (early convalescence) and 6 months (late convalescence) after initiation of an appropriate antibiotic regimen. Comparative transcriptomics identified 335 transcripts, representing 233 unique genes, with significant alterations of at least 2-fold expression in acute- or convalescent-phase blood samples from LD subjects relative to healthy donors. Acute-phase blood samples from LD subjects had the largest number of differentially expressed transcripts (187 induced, 54 repressed). This transcriptional profile, which was dominated by interferon-regulated genes, was sustained during early convalescence. 6 months after antibiotic treatment the transcriptome of LD subjects was indistinguishable from that of healthy controls based on two separate methods of analysis. Return of the LD expression profile to levels found in control subjects was concordant with disease outcome; 82% of subjects with LD experienced at least one symptom at the baseline visit compared to 43% at the early convalescence time point and only a single patient (9%) at the 6-month convalescence time point. Using the random forest machine learning algorithm, we developed an efficient computational framework to identify sets of 20 classifier genes that discriminated LD from other bacterial and viral infections. These novel LD biomarkers not only differentiated subjects with acute disseminated LD from healthy controls with 96% accuracy but also distinguished between subjects with acute and resolved (late convalescent) disease with 97% accuracy.
IMPORTANCE Lyme disease (LD), caused by Borrelia burgdorferi, is the most common tick-borne infectious disease in the United States. We examined gene expression patterns in the blood of individuals with early disseminated LD at the time of diagnosis (acute) and also at approximately 1 month and 6 months following antibiotic treatment. A distinct acute LD profile was observed that was sustained during early convalescence (1 month) but returned to control levels 6 months after treatment. Using a computer learning algorithm, we identified sets of 20 classifier genes that discriminate LD from other bacterial and viral infections. In addition, these novel LD biomarkers are highly accurate in distinguishing patients with acute LD from healthy subjects and in discriminating between individuals with active and resolved infection. This computational approach offers the potential for more accurate diagnosis of early disseminated Lyme disease. It may also allow improved monitoring of treatment efficacy and disease resolution."
***

The study authors used the same algorithms I always go to for analysis and scored well, random forest. It tends to always perform better in classification. But sometimes other algorithms perform better. Data scientists are suggested to not use just one type for all data as not all data is the same, but also some are almost as good and take much less time, depending on how many trees your algorithm is tuned to. I do want to see if these genes can be discovered that are similar to the genes they discovered and use them to predict samples from other studies, but this data is already normalized, and the method that was used was not given, so the first part of this study is an attempt at bringing back the original raw values. This is RNA blood samples, PBMC, and I do have some COVID-19 samples that are also peripheral Blood mononucleated cells type tissue, but the processing was high throughput expression profiling and not microarray. So, I would be able to split this data and see if it can predict the samples on unseen data of the testing set instead. 


```{r}
library(MASS)
library(dplyr)
library(tidyr)

library(e1071)
library(caret)
library(randomForest)
library(MASS)
#library(gbm)
library(RANN) #used in the tuning parameter of rf method of caret for 'oob' one out bag

```

```{r}
ticks <- read.delim('GSE145974_series_matrix.txt',sep='\t',header=T,
                    comment.char = '!',na.strings=c('',' ','NA'))
```

```{r}
GSM_IDs <- colnames(ticks)[2:87]
Affy_IDs <- ticks$ID_REF
```

```{r}
comments <- read.delim('GSE145974_series_matrix.txt',sep='\n',header=T,
                       na.strings=c('',' ','NA'))
```

Sample GSM IDs and description
```{r}
descriptors <- comments[27:28,]
head(descriptors)
```

```{r}
descriptors <- gsub('!','',descriptors)
descriptors <- gsub('\\t',',',descriptors)

split1 <- strsplit(descriptors[1],split=',')
type <- split1[1]
type2 <- as.data.frame(type)
colnames(type2) <- 'Sample_Title'

split2 <- strsplit(descriptors[2],split=',')
gsm <- split2[1]
gsm2 <- as.data.frame(gsm)
colnames(gsm2) <- 'Sample_GEO_Accession'

names <- cbind(type2,gsm2)
names$Sample_Title <- as.character(paste(names$Sample_Title))
names$Sample_GEO_Accession <- as.character(paste(names$Sample_GEO_Accession))
names2 <- names[-1,]
row.names(names2) <- NULL
```



```{r}
write.csv(names2,'descriptors.csv',row.names=F)
```

```{r}
descriptors2 <- read.csv('descriptors.csv',sep=',',na.strings=c('',' ','NA'),
                           header=TRUE)
head(descriptors2)
```


```{r}
descriptors2$Sample_Title
```

```{r}
descriptors2$classDisease <- c(rep('healthyControl',21),
                               rep('acuteLymeDisease',28),
                               rep('Antibodies_1month',27),
                               rep('Antibodies_6months',10))
write.csv(descriptors2,'descriptors2.csv',row.names=F)
descriptors2
```

```{r}
platform <- read.delim('GPL13667-15572.txt',sep='\t',header=T,
                       na.strings=c('',' ','NA'),
                       comment.char='#')
```


```{r}
colnames(platform)
```

```{r}
platform2 <- platform[,c(1,15)]
head(platform2,10)
```

```{r}
split3 <- strsplit(as.character(platform2$Gene.Symbol),split='///')
Gene1 <- lapply(split3,'[',1)
platform2$Gene <- as.character(paste(Gene1))
platform2$Gene <- trimws(platform2$Gene,which='both',whitespace=' ')
platform3 <- platform2[,c(1,3)]
```


```{r}
Lyme <- merge(platform3,ticks,by.x='ID',by.y='ID_REF')
```


```{r}
head(Lyme,10)
```

```{r}
noGeneSymbol <- Lyme[grep('---',Lyme$Gene),]
platform4 <- platform[,c(1,20)]
Ensembl <- merge(platform4,noGeneSymbol,by.x='ID',by.y='ID')
Ensembl2 <- Ensembl[-grep('---',Ensembl$Ensembl),]

string5 <- strsplit(as.character(paste(Ensembl2$Ensembl)),'///')
Ensembl2$EnsemblID <- as.character(paste(lapply(string5,'[',1)))
Ensembl3 <- Ensembl2[,c(90,4:89)]
colnames(Ensembl3)[1] <- 'Gene'
```


```{r}
LymeDisease <- Lyme[,-1]
```

Lets combine the Ensembl IDs data frame with the Gene Symbol data frame as they are some of the missing observations of the LymeDisease data frame with the gene symbols missing. Its only 75 out of 600 missing, but still replaces some missing values, and genecards.org will look up either gene symbol and we can grep out the Ensembl IDs with their prepended 'ENSG' ID names.
```{r}
LymeDisease2 <- LymeDisease[-grep('---',LymeDisease$Gene),]
LymeDisease3 <- rbind(LymeDisease2,Ensembl3)
```


```{r}
write.csv(LymeDisease3,'LymeDisease.csv',row.names=FALSE)
```

Our data is log2 normalized, and this means it is scaled to be between 0 and 1. There are many different ways to log2 normalize such as each x elements of a sample minus the mean of all x's in the sample, then divided by the standard deviation of all x's in the sample. Or take an element x of a sample then subtract the min(all x's in sample) and divide that by the max(all x's in sample)-min(all x's in sample). To inverse log 2 you just take 2 and raise it to the output y of log2 normalized x. To inverse the normalized method, you reverse the operations. For the first inverse, you would multiply by the std error of x then subtract the mean of x and for the second method you would multiply by the max-min and then add the min. The normalization is done before the log2 according to Dr. Quackenbush on a posted question on biostars. I want to inverse the scaling, because when doing machine learning, the data is supposed to be scaled after splitting the data into training and testing sets. And Affymetrix data has more steps for normalization as well. Lets suppose that the normalization is the second method, because I could get back the original x by converting the decimal to a fraction, but couldn't with the mean and std error method of scaling. Also if a value was zero I added 10^-8 to make it a value log2 would recognize and not quit on.

So, lets assume the formula is log2[(x-min(x))/(max(x)-min(x))]=y, then the inverse would be [2^(y)]*[max(y)-min(y)]+min(y)
```{r}
a <- LymeDisease3$GSM4340492
head(a,10)
```
Inverse step 1 to take the base 2 and raise it by y we named A.
```{r}
A <- (2^a)
head(A,10)
```
Step 2 of inverse is to inverse the standardization steps that set all values between 0 and 1. But we notice that the values above are not between 0 and 1 so they must not have been normalized with this method. And they likely weren't because Dr. Quackenbush said the values are 'background corrected,''quantile normalized,''probe summarisation (i.e. across transcripts),' and 'log (base 2) transformation.'-www.biostars.org/p/3121133/



```{r}
AA <- A*(max(A)-min(A))+min(A)
head(AA,10)
```
Those values don't look extreme, we could try to use the fractional method to get the original values back.
```{r}
AAA <- as.fractions(AA)
head(AAA,10)
```

Multiply by the maximum value in the list of de-normalized or de-standardized values.The denominators are not all common, We need a common denominator and we might need these fractions to all have common denominators. 
```{r}
maxA <- max(AAA)
A4 <- AAA*maxA
A5 <- as.numeric(A4)
head(A5,10)
```
Those values are extremely high. We were better at stopping after de-standardizing the inverse log2 of y as our x.



%%%%%%%%%%%%% demonstration of what was expected %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let me show you what I expected when using this on a different set of values.
We start with x having 10 elements, but one is a 0, and then we standardize to fit between 0 and 1.
```{r}
x <- c(1,2,3,4,5,5,43,0,23,11)
x_a <- (x-min(x))/(max(x)-min(x))
x_a
```

correct the 0 value for taking the log by adding  a very small value, otherwise it will be a NaN or log error.
```{r}
x_b <- x_a+10^-8
x_b
```
```{r}
y <- log(x_b,2)
y
```

The above is y, the log2 normalized output of x.

Lets get x back by reversing the operations.
```{r}
x_c <- 2^y
x_c
```
The above is equal to x_b, the normalized value plus the 10^-8 small value.
```{r}
x_d <- x_c-0.00000001
x_d
```
Notice that the zero is 10^-24, or a very small value, that is otherwise 0. That could be the size of the tiniest atom. 
```{r}
x_e <- x_d*(max(x_d)-min(x_d))+min(x_d)
x_e
```



```{r}
#library(MASS)
X <- as.fractions(x_e)
X


```

Notice, because its normalized the values aren't the original values, but the denominator is the max value. We can multiply by that value and get our original values back.
```{r}
X2 <- X*43
x
X2
```

We got back the original values using the second normalization method.
%%%%%%%%%%%%%%%%%%%%%%%%%%% end of demonstration %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



There are 86 samples and we would have to do this to 85 more samples or create a function that will do those steps to each column in our data frame and write it out to a file we can read back in.  We are going to forget about multiplying each entry by the max and just end the inverse log2 normalization after de-normalizing the y output vector assumedly back to x, the input vector. I tried to write a functional for loop to write this out to file, but it returned a long vector, and then when reading it in, the matrix() and the as.matrix() didn't change the 4M+ long vector (48851*86 elements as rows) into the number of rows and columns, it just kept it as a very long vector. Online, the community says that the functions decide on their own. 
```{r}
LymeMX <- LymeDisease3[,2:87]

denormalize <- function(datatable){
  for (i in LymeMX[,1:86]){
    a <- i
    A <- 2^a
    AA <- A*(max(A)-min(A))+min(A)
  write.table(AA,'lymeMX.csv',sep=',',append=TRUE,col.names=FALSE,row.names=FALSE)
  }
}
```

```{r}
if (file.exists('lymeMX.csv')){
  file.remove('lymeMX.csv')
}

denormalize(LymeMX)

lymeVector <- read.csv('lymeMX.csv',sep=',',header=F)
```

```{r}
lymeMatrix <- as.matrix(lymeVector,nrow=48851,ncol=86)
lymeMatrix2 <- as.matrix(lymeVector,nrow=48851,ncol=86)
```
Both matrices are still just one long 4,201,186 X 1 matrix. 

So, we must do this the long way, but technically copy and paste still make it somewhat fast. Just making sure to put in the right indices manually.
```{r}
lymeMx <- as.data.frame(LymeDisease3[,1])
colnames(lymeMx) <- 'gene'

lymeMx$s1 <-2^(LymeDisease3[,2])*(max(2^(LymeDisease3[,2]))-min(2^(LymeDisease3[,2])))+min(2^(LymeDisease3[,2]))
lymeMx$s2 <-2^(LymeDisease3[,3])*(max(2^(LymeDisease3[,3]))-min(2^(LymeDisease3[,3])))+min(2^(LymeDisease3[,3]))
lymeMx$s3 <-2^(LymeDisease3[,4])*(max(2^(LymeDisease3[,4]))-min(2^(LymeDisease3[,4])))+min(2^(LymeDisease3[,4]))
lymeMx$s4 <-2^(LymeDisease3[,5])*(max(2^(LymeDisease3[,5]))-min(2^(LymeDisease3[,5])))+min(2^(LymeDisease3[,5]))
lymeMx$s5 <-2^(LymeDisease3[,6])*(max(2^(LymeDisease3[,6]))-min(2^(LymeDisease3[,6])))+min(2^(LymeDisease3[,6]))
lymeMx$s6 <-2^(LymeDisease3[,7])*(max(2^(LymeDisease3[,7]))-min(2^(LymeDisease3[,7])))+min(2^(LymeDisease3[,7]))
lymeMx$s7 <-2^(LymeDisease3[,8])*(max(2^(LymeDisease3[,8]))-min(2^(LymeDisease3[,8])))+min(2^(LymeDisease3[,8]))
lymeMx$s8 <-2^(LymeDisease3[,9])*(max(2^(LymeDisease3[,9]))-min(2^(LymeDisease3[,9])))+min(2^(LymeDisease3[,9]))
lymeMx$s9 <-2^(LymeDisease3[,10])*(max(2^(LymeDisease3[,10]))-min(2^(LymeDisease3[,10])))+min(2^(LymeDisease3[,10]))
lymeMx$s10 <-2^(LymeDisease3[,11])*(max(2^(LymeDisease3[,11]))-min(2^(LymeDisease3[,11])))+min(2^(LymeDisease3[,11]))

lymeMx$s11 <-2^(LymeDisease3[,12])*(max(2^(LymeDisease3[,12]))-min(2^(LymeDisease3[,12])))+min(2^(LymeDisease3[,12]))
lymeMx$s12 <-2^(LymeDisease3[,13])*(max(2^(LymeDisease3[,13]))-min(2^(LymeDisease3[,13])))+min(2^(LymeDisease3[,13]))
lymeMx$s13 <-2^(LymeDisease3[,14])*(max(2^(LymeDisease3[,14]))-min(2^(LymeDisease3[,14])))+min(2^(LymeDisease3[,14]))
lymeMx$s14 <-2^(LymeDisease3[,15])*(max(2^(LymeDisease3[,15]))-min(2^(LymeDisease3[,15])))+min(2^(LymeDisease3[,15]))
lymeMx$s15 <-2^(LymeDisease3[,16])*(max(2^(LymeDisease3[,16]))-min(2^(LymeDisease3[,16])))+min(2^(LymeDisease3[,16]))
lymeMx$s16 <-2^(LymeDisease3[,17])*(max(2^(LymeDisease3[,17]))-min(2^(LymeDisease3[,17])))+min(2^(LymeDisease3[,17]))
lymeMx$s17 <-2^(LymeDisease3[,18])*(max(2^(LymeDisease3[,18]))-min(2^(LymeDisease3[,18])))+min(2^(LymeDisease3[,18]))
lymeMx$s18 <-2^(LymeDisease3[,19])*(max(2^(LymeDisease3[,19]))-min(2^(LymeDisease3[,19])))+min(2^(LymeDisease3[,19]))
lymeMx$s19 <-2^(LymeDisease3[,20])*(max(2^(LymeDisease3[,20]))-min(2^(LymeDisease3[,20])))+min(2^(LymeDisease3[,20]))
lymeMx$s20 <-2^(LymeDisease3[,21])*(max(2^(LymeDisease3[,21]))-min(2^(LymeDisease3[,21])))+min(2^(LymeDisease3[,21]))


lymeMx$s21 <-2^(LymeDisease3[,22])*(max(2^(LymeDisease3[,22]))-min(2^(LymeDisease3[,22])))+min(2^(LymeDisease3[,22]))
lymeMx$s22 <-2^(LymeDisease3[,23])*(max(2^(LymeDisease3[,23]))-min(2^(LymeDisease3[,23])))+min(2^(LymeDisease3[,23]))
lymeMx$s23 <-2^(LymeDisease3[,24])*(max(2^(LymeDisease3[,24]))-min(2^(LymeDisease3[,24])))+min(2^(LymeDisease3[,24]))
lymeMx$s24 <-2^(LymeDisease3[,25])*(max(2^(LymeDisease3[,25]))-min(2^(LymeDisease3[,25])))+min(2^(LymeDisease3[,25]))
lymeMx$s25 <-2^(LymeDisease3[,26])*(max(2^(LymeDisease3[,26]))-min(2^(LymeDisease3[,26])))+min(2^(LymeDisease3[,26]))
lymeMx$s26 <-2^(LymeDisease3[,27])*(max(2^(LymeDisease3[,27]))-min(2^(LymeDisease3[,27])))+min(2^(LymeDisease3[,27]))
lymeMx$s27 <-2^(LymeDisease3[,28])*(max(2^(LymeDisease3[,28]))-min(2^(LymeDisease3[,28])))+min(2^(LymeDisease3[,28]))
lymeMx$s28 <-2^(LymeDisease3[,29])*(max(2^(LymeDisease3[,29]))-min(2^(LymeDisease3[,29])))+min(2^(LymeDisease3[,29]))
lymeMx$s29 <-2^(LymeDisease3[,30])*(max(2^(LymeDisease3[,30]))-min(2^(LymeDisease3[,30])))+min(2^(LymeDisease3[,30]))
lymeMx$s30 <-2^(LymeDisease3[,31])*(max(2^(LymeDisease3[,31]))-min(2^(LymeDisease3[,31])))+min(2^(LymeDisease3[,31]))


lymeMx$s31 <-2^(LymeDisease3[,32])*(max(2^(LymeDisease3[,32]))-min(2^(LymeDisease3[,32])))+min(2^(LymeDisease3[,32]))
lymeMx$s32 <-2^(LymeDisease3[,33])*(max(2^(LymeDisease3[,33]))-min(2^(LymeDisease3[,33])))+min(2^(LymeDisease3[,33]))
lymeMx$s33 <-2^(LymeDisease3[,34])*(max(2^(LymeDisease3[,34]))-min(2^(LymeDisease3[,34])))+min(2^(LymeDisease3[,34]))
lymeMx$s34 <-2^(LymeDisease3[,35])*(max(2^(LymeDisease3[,35]))-min(2^(LymeDisease3[,35])))+min(2^(LymeDisease3[,35]))
lymeMx$s35 <-2^(LymeDisease3[,36])*(max(2^(LymeDisease3[,36]))-min(2^(LymeDisease3[,36])))+min(2^(LymeDisease3[,36]))
lymeMx$s36 <-2^(LymeDisease3[,37])*(max(2^(LymeDisease3[,37]))-min(2^(LymeDisease3[,37])))+min(2^(LymeDisease3[,37]))
lymeMx$s37 <-2^(LymeDisease3[,38])*(max(2^(LymeDisease3[,38]))-min(2^(LymeDisease3[,38])))+min(2^(LymeDisease3[,38]))
lymeMx$s38 <-2^(LymeDisease3[,39])*(max(2^(LymeDisease3[,39]))-min(2^(LymeDisease3[,39])))+min(2^(LymeDisease3[,39]))
lymeMx$s39 <-2^(LymeDisease3[,40])*(max(2^(LymeDisease3[,40]))-min(2^(LymeDisease3[,40])))+min(2^(LymeDisease3[,40]))
lymeMx$s40 <-2^(LymeDisease3[,41])*(max(2^(LymeDisease3[,41]))-min(2^(LymeDisease3[,41])))+min(2^(LymeDisease3[,41]))


lymeMx$s41 <-2^(LymeDisease3[,42])*(max(2^(LymeDisease3[,42]))-min(2^(LymeDisease3[,42])))+min(2^(LymeDisease3[,42]))
lymeMx$s42 <-2^(LymeDisease3[,43])*(max(2^(LymeDisease3[,43]))-min(2^(LymeDisease3[,43])))+min(2^(LymeDisease3[,43]))
lymeMx$s43 <-2^(LymeDisease3[,44])*(max(2^(LymeDisease3[,44]))-min(2^(LymeDisease3[,44])))+min(2^(LymeDisease3[,44]))
lymeMx$s44 <-2^(LymeDisease3[,45])*(max(2^(LymeDisease3[,45]))-min(2^(LymeDisease3[,45])))+min(2^(LymeDisease3[,45]))
lymeMx$s45 <-2^(LymeDisease3[,46])*(max(2^(LymeDisease3[,46]))-min(2^(LymeDisease3[,46])))+min(2^(LymeDisease3[,46]))
lymeMx$s46 <-2^(LymeDisease3[,47])*(max(2^(LymeDisease3[,47]))-min(2^(LymeDisease3[,47])))+min(2^(LymeDisease3[,47]))
lymeMx$s47 <-2^(LymeDisease3[,48])*(max(2^(LymeDisease3[,48]))-min(2^(LymeDisease3[,48])))+min(2^(LymeDisease3[,48]))
lymeMx$s48 <-2^(LymeDisease3[,49])*(max(2^(LymeDisease3[,49]))-min(2^(LymeDisease3[,49])))+min(2^(LymeDisease3[,49]))
lymeMx$s49 <-2^(LymeDisease3[,50])*(max(2^(LymeDisease3[,50]))-min(2^(LymeDisease3[,50])))+min(2^(LymeDisease3[,50]))
lymeMx$s50 <-2^(LymeDisease3[,51])*(max(2^(LymeDisease3[,51]))-min(2^(LymeDisease3[,51])))+min(2^(LymeDisease3[,51]))


lymeMx$s51 <-2^(LymeDisease3[,52])*(max(2^(LymeDisease3[,52]))-min(2^(LymeDisease3[,52])))+min(2^(LymeDisease3[,52]))
lymeMx$s52 <-2^(LymeDisease3[,53])*(max(2^(LymeDisease3[,53]))-min(2^(LymeDisease3[,53])))+min(2^(LymeDisease3[,53]))
lymeMx$s53 <-2^(LymeDisease3[,54])*(max(2^(LymeDisease3[,54]))-min(2^(LymeDisease3[,54])))+min(2^(LymeDisease3[,54]))
lymeMx$s54 <-2^(LymeDisease3[,55])*(max(2^(LymeDisease3[,55]))-min(2^(LymeDisease3[,55])))+min(2^(LymeDisease3[,55]))
lymeMx$s55 <-2^(LymeDisease3[,56])*(max(2^(LymeDisease3[,56]))-min(2^(LymeDisease3[,56])))+min(2^(LymeDisease3[,56]))
lymeMx$s56 <-2^(LymeDisease3[,57])*(max(2^(LymeDisease3[,57]))-min(2^(LymeDisease3[,57])))+min(2^(LymeDisease3[,57]))
lymeMx$s57 <-2^(LymeDisease3[,58])*(max(2^(LymeDisease3[,58]))-min(2^(LymeDisease3[,58])))+min(2^(LymeDisease3[,58]))
lymeMx$s58 <-2^(LymeDisease3[,59])*(max(2^(LymeDisease3[,59]))-min(2^(LymeDisease3[,59])))+min(2^(LymeDisease3[,59]))
lymeMx$s59 <-2^(LymeDisease3[,60])*(max(2^(LymeDisease3[,60]))-min(2^(LymeDisease3[,60])))+min(2^(LymeDisease3[,60]))
lymeMx$s60 <-2^(LymeDisease3[,61])*(max(2^(LymeDisease3[,61]))-min(2^(LymeDisease3[,61])))+min(2^(LymeDisease3[,61]))


lymeMx$s61 <-2^(LymeDisease3[,62])*(max(2^(LymeDisease3[,62]))-min(2^(LymeDisease3[,62])))+min(2^(LymeDisease3[,62]))
lymeMx$s62 <-2^(LymeDisease3[,63])*(max(2^(LymeDisease3[,63]))-min(2^(LymeDisease3[,63])))+min(2^(LymeDisease3[,63]))
lymeMx$s63 <-2^(LymeDisease3[,64])*(max(2^(LymeDisease3[,64]))-min(2^(LymeDisease3[,64])))+min(2^(LymeDisease3[,64]))
lymeMx$s64 <-2^(LymeDisease3[,65])*(max(2^(LymeDisease3[,65]))-min(2^(LymeDisease3[,65])))+min(2^(LymeDisease3[,65]))
lymeMx$s65 <-2^(LymeDisease3[,66])*(max(2^(LymeDisease3[,66]))-min(2^(LymeDisease3[,66])))+min(2^(LymeDisease3[,66]))
lymeMx$s66 <-2^(LymeDisease3[,67])*(max(2^(LymeDisease3[,67]))-min(2^(LymeDisease3[,67])))+min(2^(LymeDisease3[,67]))
lymeMx$s67 <-2^(LymeDisease3[,68])*(max(2^(LymeDisease3[,68]))-min(2^(LymeDisease3[,68])))+min(2^(LymeDisease3[,68]))
lymeMx$s68 <-2^(LymeDisease3[,69])*(max(2^(LymeDisease3[,69]))-min(2^(LymeDisease3[,69])))+min(2^(LymeDisease3[,69]))
lymeMx$s69 <-2^(LymeDisease3[,70])*(max(2^(LymeDisease3[,70]))-min(2^(LymeDisease3[,70])))+min(2^(LymeDisease3[,70]))
lymeMx$s70 <-2^(LymeDisease3[,71])*(max(2^(LymeDisease3[,71]))-min(2^(LymeDisease3[,71])))+min(2^(LymeDisease3[,71]))


lymeMx$s71 <-2^(LymeDisease3[,72])*(max(2^(LymeDisease3[,72]))-min(2^(LymeDisease3[,72])))+min(2^(LymeDisease3[,72]))
lymeMx$s72 <-2^(LymeDisease3[,73])*(max(2^(LymeDisease3[,73]))-min(2^(LymeDisease3[,73])))+min(2^(LymeDisease3[,73]))
lymeMx$s73 <-2^(LymeDisease3[,74])*(max(2^(LymeDisease3[,74]))-min(2^(LymeDisease3[,74])))+min(2^(LymeDisease3[,74]))
lymeMx$s74 <-2^(LymeDisease3[,75])*(max(2^(LymeDisease3[,75]))-min(2^(LymeDisease3[,75])))+min(2^(LymeDisease3[,75]))
lymeMx$s75 <-2^(LymeDisease3[,76])*(max(2^(LymeDisease3[,76]))-min(2^(LymeDisease3[,76])))+min(2^(LymeDisease3[,76]))
lymeMx$s76 <-2^(LymeDisease3[,77])*(max(2^(LymeDisease3[,77]))-min(2^(LymeDisease3[,77])))+min(2^(LymeDisease3[,77]))
lymeMx$s77 <-2^(LymeDisease3[,78])*(max(2^(LymeDisease3[,78]))-min(2^(LymeDisease3[,78])))+min(2^(LymeDisease3[,78]))
lymeMx$s78 <-2^(LymeDisease3[,79])*(max(2^(LymeDisease3[,79]))-min(2^(LymeDisease3[,79])))+min(2^(LymeDisease3[,79]))
lymeMx$s79 <-2^(LymeDisease3[,80])*(max(2^(LymeDisease3[,80]))-min(2^(LymeDisease3[,80])))+min(2^(LymeDisease3[,80]))
lymeMx$s80 <-2^(LymeDisease3[,81])*(max(2^(LymeDisease3[,81]))-min(2^(LymeDisease3[,81])))+min(2^(LymeDisease3[,81]))

lymeMx$s81 <-2^(LymeDisease3[,82])*(max(2^(LymeDisease3[,82]))-min(2^(LymeDisease3[,82])))+min(2^(LymeDisease3[,82]))
lymeMx$s82 <-2^(LymeDisease3[,83])*(max(2^(LymeDisease3[,83]))-min(2^(LymeDisease3[,83])))+min(2^(LymeDisease3[,83]))
lymeMx$s83 <-2^(LymeDisease3[,84])*(max(2^(LymeDisease3[,84]))-min(2^(LymeDisease3[,84])))+min(2^(LymeDisease3[,84]))
lymeMx$s84 <-2^(LymeDisease3[,85])*(max(2^(LymeDisease3[,85]))-min(2^(LymeDisease3[,85])))+min(2^(LymeDisease3[,85]))
lymeMx$s85 <-2^(LymeDisease3[,86])*(max(2^(LymeDisease3[,86]))-min(2^(LymeDisease3[,86])))+min(2^(LymeDisease3[,86]))
lymeMx$s86 <-2^(LymeDisease3[,87])*(max(2^(LymeDisease3[,87]))-min(2^(LymeDisease3[,87])))+min(2^(LymeDisease3[,87]))

```


We now have our suspected original x values from taking the inverse of the log2(normalized x) <pseudo code>
```{r}

head(lymeMx,10)

```
We can play around with the normalized data in some Tableau charts or this data right here that could be the raw values or close. Lets add in the actual names for our denormalized data.

```{r}
colnames(lymeMx)[2:87] <- colnames(LymeDisease3)[2:87]
```

```{r}
head(lymeMx,10)
```

Actually, these column names aren't going to do much justice to the sample identifiers in the charts, so we should align these column names up to their aliases or descriptive names. We named that table after creating it earlier as descriptors2.
```{r}
head(descriptors2,10)

```

Lets test the colnames of our denormalized and normalized data frames arethe same order as our descriptor names so we can replace the names.
```{r}
descriptors2$denormalized <- as.factor(paste(colnames(lymeMx)[2:87]))
descriptors2$normalized <- as.factor(paste(colnames(LymeDisease3)[2:87]))
descriptors2[,1:5]
```

```{r}
descriptors2$Sample_GEO_Accession==descriptors2$denormalized
descriptors2$Sample_GEO_Accession==descriptors2$normalized
```
The sample IDs are the same order as our aliases for the class they belong to. Here are our unique classes, there are four of them.
```{r}
unique(descriptors2$classDisease)
```

We can still use our shorter names or gsub() the extended names with the information we don't need. But we have to add a number to the end that makes each column name different.
```{r}

n21 <- as.character(c(1:21))
n28 <- as.character(c(1:28))
n27 <- as.character(c(1:27))
n10 <- as.character(c(1:10))
```


```{r}
descriptors2$classDisease[1:21] <- paste(descriptors2$classDisease[1:21],n21,sep='_')
descriptors2$classDisease[22:49] <- paste(descriptors2$classDisease[22:49],n28,sep='_')
descriptors2$classDisease[50:76] <- paste(descriptors2$classDisease[50:76],n27,sep='_')
descriptors2$classDisease[77:86] <- paste(descriptors2$classDisease[77:86],n10,sep='_')

```

```{r}
head(descriptors2)
```

```{r}
descriptors2$classDisease
```

```{r}
write.csv(descriptors2,'descriptors2.csv',row.names=F)
```


```{r}
LymeDisease4 <- LymeDisease3
colnames(LymeDisease4)[2:87] <- descriptors2$classDisease
lymeMx2 <- lymeMx
colnames(lymeMx2)[2:87] <- descriptors2$classDisease

```


```{r}
write.csv(LymeDisease3,'LymeDisease3.csv',row.names=FALSE)
write.csv(LymeDisease4,'LymeDisease4normalized-easynames.csv',row.names=FALSE)
write.csv(lymeMx2,'lymeMx2-denormalized-easynames.csv',row.names=FALSE)
write.csv(lymeMx,'lymeMx-denormalized-originalnames.csv',row.names=FALSE)
```

Now, we can use this data to find the mean values across samples and get the fold change values, then plot the data in Tableau.
```{r}
LymeDisease5 <- LymeDisease4 %>% group_by(Gene) %>% summarise_at(vars('healthyControl_1':'Antibodies_6months_10'),mean)
```


```{r}
lymeMx3 <- lymeMx2 %>% group_by(gene) %>% summarise_at(vars('healthyControl_1':'Antibodies_6months_10'),mean)
```


```{r}
Lyme6 <- LymeDisease5 %>% group_by(Gene) %>% 
  mutate(
    healthy_Mean = mean(healthyControl_1:healthyControl_21,na.rm=T),
    acuteLymeDisease_Mean = mean(acuteLymeDisease_1:acuteLymeDisease_28,na.rm=T),
    antibodies_1month_Mean = mean(Antibodies_1month_1:Antibodies_1month_27,na.rm=T),
    antibodies_6month_Mean = mean(Antibodies_6months_1:Antibodies_6months_10,na.rm=T)
  )
```


```{r}
tail(colnames(Lyme6),5)
```


```{r}
lymeMx4 <- lymeMx3 %>% group_by(gene) %>% 
  mutate(
    healthy_Mean = mean(healthyControl_1:healthyControl_21,na.rm=T),
    acuteLymeDisease_Mean = mean(acuteLymeDisease_1:acuteLymeDisease_28,na.rm=T),
    antibodies_1month_Mean = mean(Antibodies_1month_1:Antibodies_1month_27,na.rm=T),
    antibodies_6month_Mean = mean(Antibodies_6months_1:Antibodies_6months_10,na.rm=T)
  )
```


```{r}
tail(colnames(lymeMx4),5)
```


```{r}
lymeMx5 <- lymeMx4 %>% group_by(gene) %>% 
  mutate(acuteHealthy_foldChange=acuteLymeDisease_Mean/healthy_Mean,
    antibodies_1month_healthy_foldChange=antibodies_1month_Mean/healthy_Mean,
    antibodies_6month_healthy_foldchange=antibodies_6month_Mean/healthy_Mean)
```


```{r}
tail(colnames(lymeMx5),10)
```


```{r}
Lyme7 <- Lyme6 %>% group_by(Gene) %>% 
  mutate(acuteHealthy_foldChange=acuteLymeDisease_Mean/healthy_Mean,
    antibodies_1month_healthy_foldChange=antibodies_1month_Mean/healthy_Mean,
    antibodies_6month_healthy_foldchange=antibodies_6month_Mean/healthy_Mean)
```


```{r}
tail(colnames(Lyme7),10)
```

Our tables of unique genes grouped by genes to get their means of each gene within each sample for the duplicate genes, the added features of each class's mean gene expression per gene, and the fold change ratio of the diseased or treated to the healthy gene expression values have been created. The normalized data or the original data is the **Lyme7** data frame and the denormalized data is the **lymeMx5** data frame. Now each shrunk from 48851 genes to 19526 genes when grouping by unique genes, but now that is still a lot of genes, so lets take the gene that have the top 10 most expressed and least expressed values in both data frames by acute/healthy fold change, and the top 10 and bottom 10 of the 1month of antibodies/healthy fold change values, and finally the top 10 and bottom 10 of the 6 month of antibodies/healthy fold change values.
***
The denormalized group first:

Acute/healthy top 10 and bottom 10 genes by fold change data frame:
```{r}
acuteHealthy20 <- lymeMx5[order(lymeMx5$acuteHealthy_foldChange,
                                decreasing=T)[c(1:10,19517:19526)],]
```

One month/healthy top 10 and bottom 10 genes by fold change data frame:
```{r}
month1healthy20 <- lymeMx5[order(lymeMx5$antibodies_1month_healthy_foldChange,
                                 decreasing=T)[c(1:10,19517:19526)],]
```

Six month/healthy top 10 and bottom 10 genes by fold change data frame:
```{r}
month6healthy20 <- lymeMx5[order(lymeMx5$antibodies_6month_healthy_foldchange,
                                 decreasing=T)[c(1:10,19517:19526)],]
```


```{r}
lymeMx6 <- rbind(acuteHealthy20,month1healthy20,month6healthy20)
lymeMx7 <- lymeMx6[!duplicated(lymeMx6),]
```
There were 43 unique genes between all three fold change groups in the denormalized data out of 60 genes that were either the top 10 or bottom 10 of genes being expressed.

Now, for the normalized data:

Acute/healthy top 10 and bottom 10 genes by fold change data frame:
```{r}
acuteHealthy20b <- Lyme7[order(Lyme7$acuteHealthy_foldChange,
                                decreasing=T)[c(1:10,19517:19526)],]
```

One month/healthy top 10 and bottom 10 genes by fold change data frame:
```{r}
month1healthy20b <- Lyme7[order(Lyme7$antibodies_1month_healthy_foldChange,
                                 decreasing=T)[c(1:10,19517:19526)],]
```

Six month/healthy top 10 and bottom 10 genes by fold change data frame:
```{r}
month6healthy20b <- Lyme7[order(Lyme7$antibodies_6month_healthy_foldchange,
                                 decreasing=T)[c(1:10,19517:19526)],]
```


```{r}
Lyme8 <- rbind(acuteHealthy20b,month1healthy20b,month6healthy20b)
Lyme9 <- Lyme8[!duplicated(Lyme8),]
```
There are 33 genes unique to the normalized data, probably because this data had negative values. The scaling done to denormalize this data is probably not exactly what the true raw values are. But they should have the same number of genes, but this one has 10 less than the normalized data. We will see later which one can be split into training and testing sets with better prediction accuracy within each class and overall. 


Lets also add the gene summaries to these data frames and create a field that will give the class of each sample. This file,genecards2.R, is an R file sourced for the functions made in previous scripts. We lose one of the genes in the original data frame because it isn't in genecards.org and end up with 32 instead of 33 genes for that data frame.
```{r}
source('geneCards2.R')
```

LOC400657 (#23 in list) is a gene that genecards.org doesn't recognize and it will throw an error, so we should skip it.
```{r, eval=F}
for (i in Lyme9$Gene[1:22]){
  getSummaries2(i,'protein')
}

for (i in Lyme9$Gene[24:33]){
  getSummaries2(i,'protein')
}

```



```{r, eval=F}
getGeneSummaries('protein')
```


```{r}
summsLyme9 <- read.csv("proteinGeneSummaries_protein.csv")
```


```{r, eval=F}
for (i in lymeMx7$gene){
  getSummaries2(i,'immune')
}
```


```{r, eval=F}
getGeneSummaries('immune')
```


```{r}
summsLymeMx7 <- read.csv("proteinGeneSummaries_immune.csv")
```


```{r}
Lyme10 <- merge(summsLyme9,Lyme9,by.x='gene', by.y='Gene')
lymeMx8 <- merge(summsLymeMx7,lymeMx7, by.x='gene',by.y='gene')
```

Lets create those classes for each data frame. But first we have to tidy the data.
```{r}
Lyme11 <- gather(Lyme10, key='classSample',value='classValue',8:93)
lymeMx9 <- gather(lymeMx8,key='classSample',value='classValue',8:93)
```

```{r}
Lyme11$class <- Lyme11$classSample
Lyme11$class <- gsub('^hea.*$','healthy',Lyme11$class, perl=T)
Lyme11$class <- gsub('^acute.*$','acute Lyme Disease',Lyme11$class,perl=T)
Lyme11$class <- gsub('^.*1month.*$','1 month treatment', Lyme11$class, perl=T)
Lyme11$class <- gsub('^.*6month.*$','6 months treatment',Lyme11$class,perl=T)
```


```{r}
lymeMx9$class <- lymeMx9$classSample
lymeMx9$class <- gsub('^hea.*$','healthy',lymeMx9$class, perl=T)
lymeMx9$class <- gsub('^acute.*$','acute Lyme Disease',lymeMx9$class,perl=T)
lymeMx9$class <- gsub('^.*1month.*$','1 month treatment', lymeMx9$class, perl=T)
lymeMx9$class <- gsub('^.*6month.*$','6 months treatment',lymeMx9$class,perl=T)
```



```{r}
unique(Lyme11$gene)
```

```{r}
unique(lymeMx9$gene)

```

It looks like the genes aren't even the same genes.
```{r}
unique(Lyme11$Gene) %in% unique(lymeMx9)
```

Apparently, they are not the same genes. Its ok, maybe they still offer some information. The techniques and methods are the same to inverse what was assumed to be the normalization method, but for typical studies. In bioinformatics, with gene expression data, there is usually more to it, like trimming the bottom and top outliers, and taking the quantile normalization, then scaling. We used the standardization method of normalizing values between 0 and 1 as log2 normalized is to 
f(x)=log2[(x-min(x))/(max(x)-min(x))]=y and the inverse would be:
f(y)=2^[y*(max(y)-min(y))+min(y)]=x
So, there is some logic to this, and at some point rounded values could lose information in the numer of scientific placeholders of precision is used in calculating the inverse of the base 2 log, or the exact values for max and min of X need to be used. Reminder, when I demonstrated this earlier, the method worked using this procedure for 10 values that included a 0 where a small value was added to take the log2 of x=0 without an error, but the exact values were still decimals at the final step. To fix this they were turned to fractions, where the denominator was the max(x), and so each value multiplied by the denominator at that point returned the original x values in our list of 10. When addidng that step to the last step we used on this data to denormalize the data, the values were extremely large, approximately 10^3-10^4 larger. So we stopped before taking the fractional values. We will continue with these genes in our machine learning to see if either set makes good gene targets for pathogenesis of lyme disease by how accurately the classes of: healthy, acute disease, 1 month convalescing or developing antibodies after being given a regimen of antibiotics, and 6 months convalescing after being given antibiotics. This is temporal or time specific data, and there were some discrepencies in the study when being done, because it spanned 2 years, some patients didn't know how long they had it but if they had symptoms they were assumed to be suffering from lyme disease, like the facial paralysis or the skin lesion type marks. Also, some patients dropped out and if the study spanned two years, and only the last 6 months recorded the convalescing at 6 months then the first batches of patients in the acute phase weren't being recorded or they were actually being monitored after six months and up to two years after being given antibiotics. So we can imagine the data might be skewed for these differences or discrepencies. 

Lets write these two tables out to csv files to analyze visually in Tableau.
```{r}
write.csv(Lyme11,'LymeDisease_originalValues_foldchages32.csv',row.names=F)
```


```{r}
write.csv(lymeMx9,'LymeDisease_denormalizedValues_foldchanges43.csv',row.names=F)
```


***

Lets see what great charts were created in Tableau with this data using our de-normalized or de-standardized data.

![Tableau Dashboard of Lime Disease De-Standardized](./images/dashboardLymeDisease.png)

The link to this dashboard is at this site:https://public.tableau.com/profile/janis5126#!/vizhome/LymeDiseaseDashboardGSE145974/LymeDiseaseDashboardGSE145974?publish=yes

<a href='https://public.tableau.com/profile/janis5126#!/vizhome/LymeDiseaseDashboardGSE145974/LymeDiseaseDashboardGSE145974?publish=yes' target='blank'>Dashboard De-standardized Lyme Disease Data GSE145974</a>

In the above image and the dashboard if you click on the link above, You can see the genes to the right with the gene summaries if you hover over the text to the right of the dashboard in the 'Gene Filtering' box. It will select only the genes you select to show the median gene expression values within each class of healthy, acute lyme disease, one month after antibiotics treatment, and six months after antibiotics treatement, with varying class sizes due to changes in patient participation and methods during the study. The top chart of the warm colors is for the median gene expression values for each gene of 43 genes that were filtered from 19,000 genes as having the most or least fold change in disease or treatment to healthy ratios for all three classes with duplicates removed from the top 10 or bottom 10 genes in each class by fold change. The lower left chart with the greens is the fold change values for each gene within each class of acute lyme disease, one month of treatment, or six months of treatment compared to healthy samples by mean values of all samples in each class. The lower right chart of the purple colors is a tree map that is categorized by class and within each class each box is a gene with the average gene expression value within that class for that gene. The upper right box shows that the gene DEFA1 was selected and it is displayed in all three accompanying charts on the dashboard.

The following images are the charts that are in the dashboard above.


<a href='https://public.tableau.com/profile/janis5126#!/vizhome/FoldChangeLymeDiseaseGSE145974/FoldChangeLymeDiseaseGSE145974?publish=yes' target='blank'>link to image of the bar chart of fold change values</a>.

![Bar chart of Fold Change Values](./images/foldChangeLymeDisease.png)



<a href='https://public.tableau.com/profile/janis5126#!/vizhome/medianClassValuesLymeDiseaseGSE145974/medianClassValuesLymeDiseaseGSE145974?publish=yes' target='blank'>link to image of the bar chart of median gene expression values</a>.

![Bar chart of Median Gene Expression Values](./images/MedianValuesLymeDiseaes.png)



<a href='https://public.tableau.com/profile/janis5126#!/vizhome/TreeBoxChartLymeDiseaseGSE145974/TreeBoxChartLymeDiseaseGSE145974?publish=yes' target='blank'>link to image of the treechart of average gene expression values within each class</a>.

![Treemap chart of the average gene expression values for each gene within each of the four classes](./images/TreemapLymeDisease.png)


We will perform machine learning on this data in the upcoming additions to this post. But first we will also look at those genes that are from the original data as the most or least expressed genes in our lyme disease data obtained from NCBI with accession ID, GSE145974.

***

The original data with log2 normalized values including negative values was used to make the same or similar dashboard of charts as was done above with the de-standardized lyme disease data of GSE145974. However, because of the negative values, the treemap chart was replaced with a highlight chart as the treemap removed 45 negative values. This data took a total of 32 genes (technically 33 but one didn't have a gene summary so it was omitted), and compared the most or least expressed genes as up and down regulated in the disease (acute) or treatment (1 month or 6 months of antibiotics) to the healthy samples all as the fold change of class sample mean values per gene. 

<a href='https://public.tableau.com/profile/janis5126#!/vizhome/dashboardOriginalGSE145974/dashboardOriginalGSE145974?publish=yes' target='blank'>dashboard of the original GSE145974 top changing genes</a>

![image dashboard of the original GSE145974 top changing genes](./images/dashboardOriginal.png)

Figure 5: The above image is a dashboard of the original log2 normalized data that has negative values as well as positive values. The filter at the top right will display only the gene or genes selected if you select one then use ctrl + click on each additional gene in that 'Gene Filter' box. The other three charts will show the respective gene as it relates in the top for median gene expression values within each class and the number of samples in each class, in the lower left the fold change values for that gene in the three classes of diseased or treatment to healthy as a ratio of means, and in the lower right the highlight table with a color gradient bar of oranges and grays for the gene or genes selected. The oranges are the lowest values, or down-regulated negative gene expression values from the median, and the grays are the genes with more positive gene expression values or up regulation in each class of acute, 1 month of treatment, 6 months of treatment, or healthy. The image above will link to this dashboard to try out the different genes associated with lyme disease and treatment. I just read an article about Gigi Hadid having lyme disease since age 14 and is now 21 years old with brain fog, joint pain and stiffness, light sensitivity, headaches, anxiety, and possibly other symptoms like face paralysis related to lyme disease.



<a href='https://public.tableau.com/profile/janis5126#!/vizhome/HighlightTableOriginalGSE145974/HighlightTableOriginalGSE145974?publish=yes' target='blank'>Highlight chart of original GSE145974 data</a>

![image of highlight chart of original GSE145974 data](./images/HighlightTable.png)

Figure 6: The above image is a highlight chart of the average gene expression values within each class of acute, one month of treatment, six months of treatment, and healthy classes. This chart was used instead of the treemap chart used in the de-standardized data because it allows negative values, and treemap charts do not and would have eliminated 45 gene values across these samples. The genes are gradient color coded, so that under expressed genes in samples having negative values are reddish-orange, and those genes with more gene expression values or up regulated are gray. Colors in between the reddish-orange and gray are for genes that didn't change as much up or down in gene expression. After six months of treatments one gene is highly under expressed, ISG20, and it is in the middle of the chart in a reddish color that indicates it is the most under expressed gene or at the end of the lowest values for gene expression. The gene summary for this gene is in the dashboard that Figure 5 links to. The Entrez gene summary says Hepatitis C and Yellow fever are associated with abnormalities in this gene and its network involvements include the innate immune system. A gene with the highest up regulation is CENPF in the acute phase it is highly up regulated. The gene summary for this gene says it could possibly have some involvement with chromosome segregation during mitosis and also that it encodes a protein associated with the centromere-kinetochore complex. Also, autoantibodies in cancer patients have been found that target this gene, CENPF. And a quick online wikipedia search says autoantibodies are the antibodies your own body produces to attack your own body's proteins.



<a href='https://public.tableau.com/profile/janis5126#!/vizhome/foldChangeOriginalGSE145974/foldChangeOriginalGSE145974?publish=yes' target='blank'>Original Fold Change Values GSE145974</a>

![image of original GSE145974 fold change values](./images/foldChangeOriginal.png)

Figure 7: The image above is to the chart of original lyme disease data fold change values for each gene across all samples. It is bidirectional as is the other charts in direction or color gradient, because this data has negative values accomodating the log2 normalized data. Negative values indicate down regulation and positive values indicate up regulation or you can think suppression versus explosion if magnitude dramatic enough relative to the neighboring genes. All these genes are the most or least expressed of all genes in the data using the original values, so they should have some change visible. The gene CTXN3 is shown to be highly down regulated with -128,817 in fold change comparison of this gene in patients' average gene expression after six months of treatment compared to the healthy samples. It was down regulated on average 10^-6 approximately less than healthy samples. That is a very large magnitude and could possibly be a target gene for having the disease or antibodies. It is important to get the treatment early to avoid symptoms, but some people still have symptoms and treatment might not work well or at all is what this could be indicating. Because the other classes of acute and one month of treatment as well as healthy don't have this magnitude of down regulation at all. Keep in mind the patients in this class was nearly a third of the original sample size 10/28 of acute patients. The gene summary for this gene, CTXN3, in the dashboard says it is a protein encoding gene that Autotopagnosia and Clear Cell Adinoma are diseases associated to CTXN3. Autotopagnosia is the inability for one to identify his or her body parts or locate them on his or her body. And Clear Cell Adinoma is a vaginal/cervical cancer that is rare and usually diethylstilbestrol (DES) exposure in utero of a female's mother. The daughters of moms exposed to DES are more likely to get Clear Cell Adinoma and a gene that is highly underexpressed in our 6 months of treatment group, CTXN3, is associated to that disease. Either by having lower risk by not producing as much, or increased risk due to not producing much of it as the healthy and acute disease phase are. 



<a href='https://public.tableau.com/profile/janis5126#!/vizhome/medianOriginalGSE145974/medianOriginalGSE145974?publish=yes' target='blank'>Median Gene Expression Values original GSE145974 data</a>

![image of median gene expression values of original GSE145974 data](./images/MedianValuesOriginal.png)

Figure 8: The above image is to the bar chart that is bidirectional like Figure 7 of fold change values, but this chart is of the median gene expression values across all four samples for each gene. The number of samples in each class is also labeled on each bar. Scrolling through the genes in the chart you will see other genes like ENO1 which has a gene summary stating it encodes alpha-enolase, one of three enolase isoenzymes found in mammals. This gene is associated with an autoantigen in Hoshimoto encephalopothy, another autoimmune contributor it sounds like. We see it is dramatically under regulated in the healthy samples and also under regulated in the samples who received six months of antibiotic treatment. But in the acute phase it is up regulated almost 50% more than the healthy samples and in the acute samples it is also up regulated but by about 25% of the healthy samples. ISG20 is very highly under regulated in the 6 month class at about 10 fold the amount of the healthy class median values which is also under regulated. We saw this gene earlier in our highlight chart as being associated with yellow fever and hepatitis C as well as innate immunity network signaling. It is the most under regulated gene in all. A gene, RNF168, is also highly under regulated in the 6 month class, but the healthy class and 1 month class are up regulated in this gene by 3-4 fold more than the 6 month class by visual inspection. This gene, RNF168, has an Entrez gene summary that states it is involved in DNA Double-Strand Break (DSB) repair, and that it has mutations associated with Riddle syndrome. Wikipedia says this is a rare genetic disease that causes radiosensitivity, ImmunoDeficiency Dysmorphic  features, and learning difficulties as an acronym meaning.


***

There are a lot of different genes with useful information and they are the top genes in changes in gene expression in either data, but we still need to test these genes to see how they compare using machine learning to see how well the classifications can be predicted by these genes. We will get to that later but soon. 


Lets start the machine learning by first making the data frames with the class as the output or target feature and the samples as observations and the genes as predictors from both sets separately.

The 43 de-standardized genes will be created first then the 32 original genes that are completely different. Both are the filtered top or bottom 10 genes out of their respective 19526 unique gene sets of each class by fold change of acute/healthy, 1 month/healthy, or 6 months/healthy by means of their respective class samples.

The destandardized set. Lets just name our data sets something silly to keep track of them. **Dance is the de-standardized set and Stand** is the original log2 normalized set. 


The Dance Machine Learning set, made from the lymeMx7, not-tidied, de-standardized data frame:
```{r}
colnames(lymeMx7)
```

Lets remove the fold change and mean value features from our lymeMx7 data frame and save it as 'Dance' after we transpose it to get the unique genes as predictors and the samples as observations.
```{r}
dance <- lymeMx7[,-c(88:94)]
danceSampleNames <- colnames(dance)[2:87]

month1 <- grep('1month',danceSampleNames)
month6 <- grep('6month',danceSampleNames)
healthy <- grep('healthy',danceSampleNames)
acute <- grep('acute',danceSampleNames)

class <- danceSampleNames
class[month1] <- '1 month'
class[month6] <- '6 months'
class[healthy] <- 'healthy'
class[acute] <- 'acute'


danceGeneNames <- dance$gene
Dance <- as.data.frame(t(dance[,-1]))
colnames(Dance) <- danceGeneNames
Dance$class <- class
Dance2 <- Dance[,c(44,1:43)]
head(Dance2)
```

We have our machine learning ready data frame of de-standardized genes, and will be using the target, class, for predictions. We could use all 43 genes or just take those genes in the visualizations that we saw had very peculiar fold change values like in the 6 months of treatment or acute stages. Or we could test both. Might as well test bost as we will see how likely these genes are in predicting an acute disease stage, treatment time, or healthy class by blood analysis.

Lets refresh our memories on what those genes were. We put them in our notes on the visualizations above for the de-standardized Tableau charts. We might miss some, as those were scanned visuals, so I am going to make a list of those genes that have noticeable shifts in gene expression or fold change values compared to the other classes and make that are peculiar set of genes. We could even divide those genes up into the ones up or down regulated in the 6 month or the acute stage only or even the healthy samples only. I will revisit that <a href='https://public.tableau.com/profile/janis5126#!/vizhome/LymeDiseaseDashboardGSE145974/LymeDiseaseDashboardGSE145974?publish=yes' target='blank'>dashboard</a> and select the genes from the filter and compare across all charts available and bring back the findings here. We'll call that set Dance-odd6, Dance-oddAcute, or Dance-oddHealthy. Possibly Dance-odd1, but I didn't notice anything the first quick scan through the genes. There are only 43, so it shouldn't be a problem.

Using the fold change values:

- acute up, decreasing order of up is 1 month, healthy, 6 months
	
- up acute

monotonically decreasing from acute -> 1 month -> 6 months:

BPI  
CAMP 
CEACAM8 
CTSG 
DEFA1 
DEFA4 
DHX58 
FCGR3B  
GAPT 
GZMH 
HBG1 
HLA-DRB4 
KIR2DL3 
KIR2DS1 
LCN2 
LSM2 
LTF 
MS4A3 
POLR21 
RGS18 
S100B 
TNFSF10 

- more up regulated in acute, drops in 1month then up regulated to approximately 1/2 acute in 6 months:

THBD
CHI3L1
SERPINB2

-up in acute and down 1/2 in 1 month with slight increase in 6 months by approx 5%

TXNL4A

- up reg in acute, drops in 1 month then almost up reg same amount in 6 months as in acute

HTR3C

%%%%%
- up 6 months

- starts up in acute, drops in 1 month then up in 6 months close to acute levels

AREG
BEST1
KIAA1245
LIPN
OR2B11

- starts mid-level in acute, then stays about the same in 1 month, then noticeably up in 6 months about two fold as in acute

CKMT1B

- starts low in acute, drops a little in 1 month, then up in 6 months much more noticeably then in the acute by 5-10 fold acute levels

IL1B 
MUC12
MYOM2
OLR1
THBS1
TSIX
XIST

- starts low, stays same approx in 1 month, then much more noticeably up in 6 months 5-10 fold

FSIP1


Few observations, many of the genes are monotonically decreasing from acute to 1 month to 6 months in gene expression levels, where they start high in the acute stage, then decrease gradually in 1 month:
 **BPI**, **CAMP, CEACAM8, CTSG,  DEFA1, DEFA4, DHX58, FCGR3B, GAPT, GZMH, HBG1,** **HLA-DRB4,KIR2DL3, KIR2DS1, LCN2, LSM2, LTF, MS4A3, POLR21, RGS18, S100B, TNFSF10** 
 
, and more decrease in 6 months. The we have some odd genes , **THBD, CHI3L1, and SERPINB2** in the acute up regulated genes that don't behave this way, but they indicate that maybe treatment is working, because they start high in acute, then drop in 1 month of treatment, and then increase almost to half the same levels as in the acute phase after 6 months of treatment. Also, **TXNL4A**, drops in 1 month and stays about the same after 6 months. And another gene, **HTR3C**, drops in 1 month, then increases to almost same acute levels in 6 months. Lets make those lists of the acute, then the lists for the 6 months genes.
```{r}
#monotonically decreasing 
Acute_md <- c('BPI', 'CAMP', 'CEACAM8', 'CTSG',  'DEFA1', 'DEFA4', 'DHX58', 'FCGR3B', 'GAPT', 'GZMH', 'HBG1', 'HLA-DRB4','KIR2DL3', 'KIR2DS1', 'LCN2', 'LSM2', 'LTF', 'MS4A3', 'POLR21', 'RGS18', 'S100B', 'TNFSF10')

#high in acute, drop after 1 month, then half as high as acute after 6 months
Acute_mayWork <- c('THBD', 'CHI3L1','SERPINB2')

#odd ones in acute, starts high in acute, then drops, and increases slightly in 6 months
Acute_dropsThenUpslightly <- 'TXNL4A'
Acute_dropsReturnsSame <- 'HTR3C'


```


For the six months of treatment, genes that were noticeably increased after 6 months compared to the acute stage before treatment, none monotonically increased from acute levels, to 1 month of treatment levels, to six months of treatment levels. But some did drop in 1 month, then increase in 6 months to levels much higher by 5-10 fold than the acute levels, **IL1B, MUC12, MYOM2, OLR1, THBS1, TSIX, XIST**. A few genes start lower, stays lower, then increase 5-10 fold of the acute level in 6 months, **FSIP1 and CKMT1B**. And there are those genes that are more up regulated after 6 months of treatment, but only slightly more than the acute phase and after decreasing in the 1 month of treatment phase, **AREG, BEST1, KIAA1245, LIPN, and OR2B11**. Lets now make those lists to show genes that are more up regulated in the 6 month samples.
```{r}
month6_5foldup <- c('IL1B', 'MUC12', 'MYOM2', 'OLR1', 'THBS1', 'TSIX', 'XIST')
month6_5foldupStartLow <- c('FSIP1','CKMT1B')
month6_upMoreThanAcute <- c('AREG', 'BEST1', 'KIAA1245', 'LIPN', 'OR2B11')
```

Now that we have our lists, lets see about those data frames for the seven different groups of gene anomolies or similarities. The following are our ML ready dataframes for our seven groups in our de-standardized Lyme disease data.
```{r}
Acute_md_DF <- Dance2[,colnames(Dance2) %in% Acute_md]
Acute_md_DF$class <- Dance2$class

Acute_mayWork_DF <- Dance2[,colnames(Dance2) %in% Acute_mayWork]
Acute_mayWork_DF$class <- Dance2$class

Acute_dropsThenUpslightly_DF <- data.frame(TXNL4A=Dance2[,colnames(Dance2) %in% Acute_dropsThenUpslightly], row.names=row.names(Dance2))
Acute_dropsThenUpslightly_DF$class <- Dance2$class

Acute_dropsReturnsSame_DF <- data.frame(HTR3C=Dance2[,colnames(Dance2) %in%  Acute_dropsReturnsSame],row.names=row.names(Dance2))
Acute_dropsReturnsSame_DF$class <- Dance2$class

month6_5foldup_DF <- Dance2[,colnames(Dance2) %in% month6_5foldup]
month6_5foldup_DF$class <- Dance2$class

month6_5foldupStartLow_DF <- Dance2[,colnames(Dance2) %in% month6_5foldupStartLow]
month6_5foldupStartLow_DF$class <- Dance2$class

month6_upMoreThanAcute_DF <- Dance2[,colnames(Dance2) %in% month6_upMoreThanAcute]
month6_upMoreThanAcute_DF$class <- Dance2$class

```

Great, now we need to run through each of these 7 data frames and split into separate training and testing sets, and test a machine learning algorithm on. I tend to always use random forest to start with, or caret's rpart. 

Lets make sure we keep the same samples in our testing set and training set for each group to test machine learning algorithm(s) on. Lets keep the standard 70% training set and 30% testing set using a random sampling of our classes.
```{r}
set.seed(34567)
train <- sample(1:86,.7*86)
training <- class[train]
testing <- class[-train]
t <- data.frame(train = training)
ts <- data.frame(test= testing)

```


```{r}
t %>% group_by(train) %>% count(train)
```


```{r}
ts %>% group_by(test) %>% count(test)
```

We can see we have a fair share of samples in our training set and at least one of each class in our testing set to make predictions based on the model we train. Lets keep these same samples in each of our 8 groups to classify with. Lets make our 8 training and testing sets with our indices labeled 'train' and note the numeric labeling of each correspongs to their data frame:

Training/Testing split 1: Acute_md_DF
Training/Testing split 2: Acute_mayWork_DF
Training/Testing split 3: Acute_dropsThenUpslightly_DF
Training/Testing split 4: Acute_dropsReturnsSame_DF
Training/Testing split 5: month6_5foldup_DF
Training/Testing split 6: month6_5foldupStartLow_DF
Training/Testing split 7: month6_upMoreThanAcute_DF
Training/Testing split 8: Dance2

```{r}
training1 <- Acute_md_DF[train,]
testing1 <- Acute_md_DF[-train,]
training2 <- Acute_mayWork_DF[train,]
testing2 <- Acute_mayWork_DF[-train,]
training3 <- Acute_dropsThenUpslightly_DF[train,]
testing3 <- Acute_dropsThenUpslightly_DF[-train,]
training4 <- Acute_dropsReturnsSame_DF[train,]
testing4 <- Acute_dropsReturnsSame_DF[-train,]
training5 <- month6_5foldup_DF[train,]
testing5 <- month6_5foldup_DF[-train,]
training6 <- month6_5foldupStartLow_DF[train,]
testing6 <- month6_5foldupStartLow_DF[-train,]
training7 <- month6_upMoreThanAcute_DF[train,]
testing7 <- month6_upMoreThanAcute_DF[-train,]
training8 <- Dance2[train,]
testing8 <- Dance2[-train,]

```

Lets make a function specific to our data frames to return the precision, recall, and accuracy of these four classes. I actually made this in a previous script,monotonicGenes.Rmd, when testing the COVID-19 samples with GSE152418 that also had four classes to classify. But those classes were healthy, moderate, severe, or ICU grades of severity of Covid19. Actually, I found out later, that the convalescent class was its own class even though it was only one sample. So there should have been five classes. But no need to alter that function now. There is also some other packages or in the caret package, that I never use that can return the precision and recall, but i don't think as a confusion matrix. I thought the convalescent class was mislabeled, so had it relabeled as healthy, since the models pedicted it as such. I didn't find out until this study, when the summary of this study, GSE145974, used 'convalesced' blood after 1 and 6 months of antibiotics, that the sample in GSE152418 was likely its own class. I assumed it was  identifying the source of its patient sample,because another previous study on Rheumatoid Arthritis (RA), GSE151161, did use convalescent patients, and it preceded the analysis on GSE152418. Typically in research, you need a client consent and informed consent from people who aren't incarcerated or in the care of another person or facility,because it violates the human research subjects guidelines for ethical research and not victimizing vulnerable populations or culpabe and incoherant populations. This stems from research that was criminal in the Tuskegee hospital on injecting black populations with syphilis or polio vaccines on inmates in other studies for some small reward or break from their punishment or lowered/free cost clinic for medical treatment. Any researcher knows this, especially if they are funded by government agencies. Also, due to the Nazi research done on Jewish victims during World War 2, the Nuremberg Code, was created, as well as later the Belmont report. "The Nuremberg Code states that "the voluntary consent of the human subject is absolutely essential" and it further explains the details implied by this requirement: capacity to consent, freedom from coercion, no penalty for withdrawal, and comprehension of the risks and benefits involved."-The Nuremberg Code, taken from a resource for getting certified in understanding compliance with human research experiments as part of my graduate research project this had to be completed. The agency who provided this, similar to HIPPA compliance for healthcare providers, is <a href='https://about.citiprogram.org/en/homepage/' target='blank'>CITI</a>. 


```{r}
precisionRecallAccuracy <- function(df){
  
 colnames(df) <- c('pred','type')
  df$pred <- as.character(paste(df$pred))
  df$type <- as.character(paste(df$type))
  
 classes <- unique(df$type)
 
 class1a <- as.character(paste(classes[1]))
 class2a <- as.character(paste(classes[2]))
 class3a <- as.character(paste(classes[3]))
 class4a <- as.character(paste(classes[4]))
 
  #correct classes
  class1 <- subset(df, df$type==class1a)
  class2 <- subset(df, df$type==class2a)
  class3 <- subset(df, df$type==class3a)
  class4 <- subset(df, df$type==class4a)
  
  #incorrect classes
  notClass1 <- subset(df,df$type != class1a)
  notClass2 <- subset(df,df$type != class2a)
  notClass3 <- subset(df,df$type != class3a)
  notClass4 <- subset(df, df$type != class4a)
  
  #true positives (real positives predicted positive)
  tp_1 <- sum(class1$pred==class1$type)
  tp_2 <- sum(class2$pred==class2$type)
  tp_3 <- sum(class3$pred==class3$type)
  tp_4 <- sum(class4$pred==class4$type)
  
  #false positives (real negatives predicted positive)
  fp_1 <- sum(notClass1$pred==class1a)
  fp_2 <- sum(notClass2$pred==class2a)
  fp_3 <- sum(notClass3$pred==class3a)
  fp_4 <- sum(notClass4$pred==class4a)
  
  #false negatives (real positive predicted negative)
  fn_1 <- sum(class1$pred!=class1$type)
  fn_2 <- sum(class2$pred!=class2$type)
  fn_3 <- sum(class3$pred!=class3$type)
  fn_4 <- sum(class4$pred!=class4$type)
  
  #true negatives (real negatives predicted negative)
  tn_1 <- sum(notClass1$pred!=class1a)
  tn_2 <- sum(notClass2$pred!=class2a)
  tn_3 <- sum(notClass3$pred!=class3a)
  tn_4 <- sum(notClass4$pred!=class4a)
  
  
  #precision
  p1 <- tp_1/(tp_1+fp_1)
  p2 <- tp_2/(tp_2+fp_2)
  p3 <- tp_3/(tp_3+fp_3)
  p4 <- tp_4/(tp_4+fp_4)
  
  p1 <- ifelse(p1=='NaN',0,p1)
  p2 <- ifelse(p2=='NaN',0,p2)
  p3 <- ifelse(p3=='NaN',0,p3)
  p4 <- ifelse(p4=='NaN',0,p4)
  
  #recall
  r1 <- tp_1/(tp_1+fn_1)
  r2 <- tp_2/(tp_2+fn_2)
  r3 <- tp_3/(tp_3+fn_3)
  r4 <- tp_4/(tp_4+fn_4)
  
  r1 <- ifelse(r1=='NaN',0,r1)
  r2 <- ifelse(r2=='NaN',0,r2)
  r3 <- ifelse(r3=='NaN',0,r3)
  r4 <- ifelse(r4=='NaN',0,r4)
  
  #accuracy
  ac1 <- (tp_1+tn_1)/(tp_1+tn_1+fp_1+fn_1)
  ac2 <- (tp_2+tn_2)/(tp_2+tn_2+fp_2+fn_2)
  ac3 <- (tp_3+tn_3)/(tp_3+tn_3+fp_3+fn_3)
  ac4 <- (tp_4+tn_4)/(tp_4+tn_4+fp_4+fn_4)
  
  table <- as.data.frame(rbind(c(class1a,p1,r1,ac1),
                         c(class2a,p2,r2,ac2),
                         c(class3a,p3,r3,ac3),
                         c(class4a,p4,r4,ac4)))
  
  colnames(table) <- c('class','precision','recall','accuracy')
  acc <- (sum(df$pred==df$type)/length(df$type))*100
  cat('accuracy is: ',as.character(paste(acc)),'%')
  return(table)
  
  
}
```


Lets start with the first group of genes using 
Training/Testing 1:
```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfMod1 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training1),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF1 <- predict(rfMod1, testing1)

predDF1 <- data.frame(predRF1, type=testing1$class)
predDF1
```


```{r}
pra1 <- precisionRecallAccuracy(predDF1)
pra1
```


That set wasn't so great. Lets run through the other 7 sets using the same format and compare the results at the end.

Training/Testing 2:
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod2 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training2),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF2 <- predict(rfMod2, testing2)

predDF2 <- data.frame(predRF2, type=testing2$class)
predDF2
```


```{r}
pra2 <- precisionRecallAccuracy(predDF2)
pra2
```


Training/Testing 3:
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod3 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF3 <- predict(rfMod3, testing3)

predDF3 <- data.frame(predRF3, type=testing3$class)
predDF3
```


```{r}
pra3 <- precisionRecallAccuracy(predDF3)
pra3
```


Training/Testing 4:
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod4 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training4),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF4 <- predict(rfMod4, testing4)

predDF4 <- data.frame(predRF4, type=testing4$class)
predDF4
```


```{r}
pra4 <- precisionRecallAccuracy(predDF4)
pra4
```


Training/Testing 5:
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod5 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training5),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF5 <- predict(rfMod5, testing5)

predDF5 <- data.frame(predRF5, type=testing5$class)
predDF5
```


```{r}
pra5 <- precisionRecallAccuracy(predDF5)
pra5
```


Training/Testing 6:
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod6 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training6),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF6 <- predict(rfMod6, testing6)

predDF6 <- data.frame(predRF6, type=testing6$class)
predDF6
```


```{r}
pra6 <- precisionRecallAccuracy(predDF6)
pra6
```


Training/Testing 7:
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod7 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training7),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF7 <- predict(rfMod7, testing7)

predDF7 <- data.frame(predRF7, type=testing7$class)
predDF7
```


```{r}
pra7 <- precisionRecallAccuracy(predDF1)
pra7
```


Training/Testing 8:
```{r, error=FALSE, message=FALSE, warning=FALSE}
rfMod8 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training8),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF8 <- predict(rfMod8, testing8)

predDF8 <- data.frame(predRF8, type=testing8$class)
predDF8
```


```{r}
pra8 <- precisionRecallAccuracy(predDF1)
pra8
```

The seed for randomness within the computations of this operating system and R has to be set before the models, because running different times after setting the seed when generating the random indices of the train vector didn't work for the model generation. I reran 3-5 times and got inconsistent results unless using the set.seed before the 8 models were ran. It is supposed to work only once, and generate the same results everytime. But in either case it represents how the random forest works by randomly selecting samples within the sampels to test an ensemble of models and trees. This current seed still kept the 1st, 7th, and 8th groups as the highest scoring in accuracy.
Overall accuracy was not good for any of the groups where it ranged from 19-54% accuracy in predicted being the same as the actual type. But there was class accuracy differences that could best be compared by combining the precision and recall accuracy tables then adding in a feature to identify which model the result came from. Note that the worst group for prediction accuracy was group 6, and the three best groups with 54% accuracy were groups 1, 7, and 8. Worst set of genes to keep as target genes for Lyme Disease are... 
These are the groups by gene behaviors in fold change of diseased or treated mean values compared to healthy mean values:
Training/Testing split 1: Acute_md_DF
Training/Testing split 2: Acute_mayWork_DF
Training/Testing split 3: Acute_dropsThenUpslightly_DF
Training/Testing split 4: Acute_dropsReturnsSame_DF
Training/Testing split 5: month6_5foldup_DF
Training/Testing split 6: month6_5foldupStartLow_DF
Training/Testing split 7: month6_upMoreThanAcute_DF
Training/Testing split 8: Dance2

So, without tuning our models or testing other algorithms, we can assume from this point on, all the 43 genes are best, as well as the set of genes with more up regulation after 6 months than in the acute phase, and the set of genes with the monotoncially decreasing gene values from acute to one month of treatment to six months of treatment with the acute phase having the highest gene expression values. The other genes are possibly noisy or add noise to our data that prevents the model from classifying greatly. But lets see if any of the sets did have better recall or precision in a class by class prediction accuracy first, before attempting to tune our random forest models.Also note, that I omitted the preprocessing step in the model training to begin with and then added it in and improved the range from a best score of 34% to a best score in overall accuracy of 50%.
```{r}
pra_all <- rbind(pra1,pra2,pra3,pra4,pra5,pra6,pra7,pra8)
pra_all$GroupMembership <- c(rep(1,4),
                             rep(2,4),
                             rep(3,4),
                             rep(4,4),
                             rep(5,4),
                             rep(6,4),
                             rep(7,4),
                             rep(8,4))
pra_all2 <- pra_all %>% group_by(class) %>% mutate(max=
                  ifelse(accuracy==max(as.numeric(paste(accuracy))),'max','not max'))
max <- subset(pra_all2, pra_all2$max=='max')
max
```

We can see from the above chart of class membership accuracies that some other groups also did make good gene targets for some classes. Group 2 and group 3 had the best accuracy in predicting 1 or 6 months for group 3 and only the 1 month class for group 2. The 1st, 7th, and 8th groups were better at predicting the healthy and acute class memberships. We had fewer of the 6 month class, but many 1 month samples, yet that class for 1 month didn't have any noticeable changes in our 43 genes to distinguish with the random forest classification. We could try more trees or tuning the model to see if there is an improvement. These models were fast and that was likely due to the number of trees being small. 
Lets use the randomForest package and its randomForest() to tune our model and test our same 8 groups.
```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training1) <- gsub('-','',colnames(training1))
colnames(testing1) <- gsub('-','',colnames(testing1))
testing1$class <- as.factor(paste(testing1$class))
training1$class <- as.factor(paste(training1$class))
RF1 <- randomForest(class ~ ., data=training1, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict1 <- predict(RF1,testing1)
predict1df <- data.frame(predict1, type=testing1$class)
predict1df

```


```{r}
PRA1 <- precisionRecallAccuracy(predict1df)
PRA1
```


```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training2) <- gsub('-','',colnames(training2))
colnames(testing2) <- gsub('-','',colnames(testing2))
testing2$class <- as.factor(paste(testing2$class))
training2$class <- as.factor(paste(training2$class))
RF2 <- randomForest(class ~ ., data=training2, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict2 <- predict(RF2,testing2)
predict2df <- data.frame(predict2, type=testing2$class)
predict2df

```


```{r}
PRA2 <- precisionRecallAccuracy(predict2df)
PRA2
```


```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training3) <- gsub('-','',colnames(training3))
colnames(testing3) <- gsub('-','',colnames(testing3))
testing3$class <- as.factor(paste(testing3$class))
training3$class <- as.factor(paste(training3$class))
RF3 <- randomForest(class ~ ., data=training3, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict3 <- predict(RF3,testing3)
predict3df <- data.frame(predict3, type=testing3$class)
predict3df

```


```{r}
PRA3 <- precisionRecallAccuracy(predict3df)
PRA3
```

```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training4) <- gsub('-','',colnames(training4))
colnames(testing4) <- gsub('-','',colnames(testing4))
testing4$class <- as.factor(paste(testing4$class))
training4$class <- as.factor(paste(training4$class))
RF4 <- randomForest(class ~ ., data=training4, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict4 <- predict(RF4,testing4)
predict4df <- data.frame(predict4, type=testing4$class)
predict4df

```


```{r}
PRA4 <- precisionRecallAccuracy(predict4df)
PRA4
```


```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training5) <- gsub('-','',colnames(training5))
colnames(testing5) <- gsub('-','',colnames(testing5))
testing5$class <- as.factor(paste(testing5$class))
training5$class <- as.factor(paste(training5$class))
RF5 <- randomForest(class ~ ., data=training5, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict5 <- predict(RF5,testing5)
predict5df <- data.frame(predict5, type=testing5$class)
predict5df

```


```{r}
PRA5 <- precisionRecallAccuracy(predict5df)
PRA5
```


```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training6) <- gsub('-','',colnames(training6))
colnames(testing6) <- gsub('-','',colnames(testing6))
testing6$class <- as.factor(paste(testing6$class))
training6$class <- as.factor(paste(training6$class))
RF6 <- randomForest(class ~ ., data=training6, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict6 <- predict(RF6,testing6)
predict6df <- data.frame(predict6, type=testing6$class)
predict6df

```


```{r}
PRA6 <- precisionRecallAccuracy(predict6df)
PRA6
```

```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training7) <- gsub('-','',colnames(training7))
colnames(testing7) <- gsub('-','',colnames(testing7))
testing7$class <- as.factor(paste(testing7$class))
training7$class <- as.factor(paste(training7$class))
RF7 <- randomForest(class ~ ., data=training7, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict7 <- predict(RF7,testing7)
predict7df <- data.frame(predict7, type=testing7$class)
predict7df

```


```{r}
PRA7 <- precisionRecallAccuracy(predict7df)
PRA7
```


```{r}
#an error with hyphen in HLA-DRB4, so we will omit it in the testing and training set
set.seed(4567)
colnames(training8) <- gsub('-','',colnames(training8))
colnames(testing8) <- gsub('-','',colnames(testing8))
testing8$class <- as.factor(paste(testing8$class))
training8$class <- as.factor(paste(training8$class))
RF8 <- randomForest(class ~ ., data=training8, 
                    importance=TRUE, nodesize=2, ntree=400,mtry=3)
```


```{r}
predict8 <- predict(RF8,testing8)
predict8df <- data.frame(predict8, type=testing8$class)
predict8df

```


```{r}
PRA8 <- precisionRecallAccuracy(predict8df)
PRA8
```

```{r}
PRA_all <- rbind(PRA1,PRA2,PRA3,PRA4,PRA5,PRA6,PRA7,PRA8)
PRA_all$groupMembership <- c(rep(1,4),
                             rep(2,4),
                             rep(3,4),
                             rep(4,4),
                             rep(5,4),
                             rep(6,4),
                             rep(7,4),
                             rep(8,4))
PRA_all2 <- PRA_all %>% group_by(class) %>% mutate(max=
    ifelse(accuracy==max(as.numeric(paste(accuracy))),'max','not max'))
max2 <- subset(PRA_all2, PRA_all2$max=='max')
max2
```

The accuracy wasn't as great using the randomForest() instead of caret's built in random forest function. And we did still see group 8 or all 43 genes score the best or with the best. Group 5 made a class prediction best score that it didn't in the other model. Group 5 scored the best on class '1 month', and group 8 scored the best on the same class but only in accuracy, becasue group 5 had better recall and precision than Group 8 on that class. Group 8 did score 100% accuracy in the '6 months' class in recall, precision, and total accuracy. Recall that our sets are split with the same share of training and testing samples and that there were 8 samples of the 6 months class to train our model and 2 to predict in the testing set with our model. Group 8 got all relevant 6 month class samples in the testing set (precision) and no other samples were misclassified as the 6 months class (recall). The 'healthy' class was also correctly predicted by Group 8 with 81% accuracy where the precision is 71% and recall was 63%. The acute class was predicted best by group 5 with 58% accuracy, 30% recall (misclassified 70%), and 43% precision (didn't find 57%).

We could test out more algorithms or we could test out the original data of 32 completely different genes. and go through the same process of grouping the genes by those gene fold change ratios that we saw in our 7 groups above. We could also test out a data set of those genes in our top performing groups, 1,8, and 2,3,5,or 7 with groups 4 and 6 not being a better performer at any class prediction or overall accuracy. Those groups again are:

Training/Testing split 1: Acute_md_DF
Training/Testing split 2: Acute_mayWork_DF
Training/Testing split 3: Acute_dropsThenUpslightly_DF
Training/Testing split 4: Acute_dropsReturnsSame_DF
Training/Testing split 5: month6_5foldup_DF
Training/Testing split 6: month6_5foldupStartLow_DF
Training/Testing split 7: month6_upMoreThanAcute_DF
Training/Testing split 8: Dance2

where md is monotonically decreasing from acute -> month 1 -> month 6 and Dance2 is all genes. There weren't any monotonically increasing genes, all the remaining genes started higher than the 1 month class then increased to a level close to the acute levels, just under the acute levels, slightly more than the acute levels, or much higher than the acute levels. All 43 genes (Dance2), the monotonically decreasing genes, and all groups except for group 4 and 5 can be used. But really we are just picking the ones that aren't useful from group 8. Groups 4 and 6, with group 6 seeming to always score the minimum accuracy will be in our data set to test our models on. This means neither of the genes that return to similar levels from acute to the 6th month levels or the genes from the group that starts low in the acute phase but end up increasing to about five fold the acute levels by month 6 will be used. And I would have thought those genes would be indicative of the class. We should just make two data sets, where one is Groups 4 and 6, and the other is groups 1,2,3,5, and 7. Because group 8 is all the genes in the set. Either set could have some noise. These are fold change values of the mean values across all samples. It is possible to go back to the dashboard and find some outlier samples that skew the gene values from group 4 and 6. Lets see what those genes are again. Acute_dropsReturnsSame, and month6_5foldupStartLow are those gene lists made earlier.

```{r}
poorPathogenesisTargets <- c(month6_5foldupStartLow, Acute_dropsReturnsSame)
poorPathogenesisTargets
```

We need to go back to the <a href='https://public.tableau.com/profile/janis5126#!/vizhome/LymeDiseaseDashboardGSE145974/LymeDiseaseDashboardGSE145974?publish=yes' target='blank'>dashboard</a> and see if FSIP1, CKMT1B, or HTR3C have some samples that are skewing their gene expression values greatly. 

I actually didn't post the individual gene expression values up in a chart on Tableau, so I just loaded one that shows there are some samples skewing the data with those three genes, FSIP1, CKMT1B, and HTR3C. I have decided to backtrack and see if I take the median fold change values instead of the mean across all samples if the results will be better. 

<a href='https://public.tableau.com/profile/janis5126#!/vizhome/genesAcrossEachSampleGSE145974/genesAcrossEachSampleGSE145974?publish=yes' target='blank'>individual samples' gene expression values across all four classes</a>.

![FSIP1, CKMT1B, and HTR3C spread across all samples](./images/SampleGeneExpressionsBarChart.png)

Figure 9: I added the sample chart to see the groups of individual samples within each class of healthy, acute lyme disease, one month of antibiotic treatment, and six months of antibiotics treatments, after realizing, some of the genes' fold change values are skewing the data greatly. And we can see in the above image of this chart (linked to through the image) that these samples in this set of genes that skewed our data when running some machine learning algorithms were samples: sample 7 of the acute class samples, sample 12 of the 1 month class samples, sample 10 of the 6 month class samples, and samples 1, 11, and 12 of the healthy class samples. I want to remove these samples and run some machine learning on the set, or just take the median sample values instead when deriving the fold change values. 


***

I want to backtrack at this point and use the median values by switching to a new document to test the median, and referencing back to it in this document, with the machine learning results.

I did that work on the median sample values and dropped those six classes that seemed to skew the data, but the results weren't better and had a best score of 42% accuracy, where here the best score was 54% accuracy so far. We still need to test the machine learning results on the original data that destandardization wasn't made to. But to access the median sample values as fold changes and the results with the six outlier samples in the mean sample derived fold change data before getting the median derived fold changes, it is on rpubs as [part 2 of the Lyme Disease Ticks](https://rpubs.com/janisharris/lymeDiseaeGSE145974part2median6samplesDropped) document.


I took out six samples that were skewed in this set, but never tested if taking those samples out of this data would improve the classification accuracy in this data. We can do that fast with our testing and training sets. Lets use all the data of training and testing set 8. 
```{r}
row.names(training8)
```

sample 7 of the acute class samples, sample 12 of the 1 month class samples, sample 10 of the 6 month class samples, and samples 1, 11, and 12 of the healthy class samples.

Check back for machine learning on the original data. 
```{r}
skewSamples <- c('Antibodies_6months_10','Antibodies_1month_12','acuteLymeDisease_7',
                 'healthyControl_1','healthyControl_11','healthyControl_12')
```


```{r}
sort(row.names(training8))
```


```{r}
sort(skewSamples)
```


```{r}
skewSamples %in% row.names(training8)
```


```{r}
skewSamples %in% row.names(testing8)
```


```{r}
dim(training8);dim(testing8)
```


```{r}
training8b <- subset(training8,!(row.names(training8) %in% skewSamples))
testing8b <- subset(testing8, !(row.names(testing8) %in% skewSamples))
```


```{r}
skewSamples %in% row.names(training8b)
skewSamples %in% row.names(testing8b)
dim(training8b);dim(testing8b)
```

Now, we can see if there is an improvement in accuracy in machine learning prediction.
Training/Testing 1:
```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfMod8b <- train(class~., method='rf', 
               na.action=na.pass,
               data=(training8b),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRF8b <- predict(rfMod8b, testing8b)

predDF8b <- data.frame(predRF8b, type=testing8b$class)
predDF8b
```


```{r}
pra8b <- precisionRecallAccuracy(predDF8b)
pra8b
```

The accuracy is 47.6% for all genes used on all samples except the six skewed ones. The highest Group 8 scored was 54% earlier. Maybe we could try training more samples and having a smaller test set to predict the classifications? Or making the classes more balanced. Lets see what are class counts are in each set. The healthy class isn't oo bad at 80% precision, it only missed 20% of the healthy samples, but the recall is 67% on the healthy class, meaning it misclassified some samples as healthy. And the recall was 100% on the 6 month class, when there is only one 6 month class in the testing set, it correctly predicted the only class as 6 months out of the total number of classes there are (recall) is 100% but also incorrectly predicted one of the acute classes as a 6 month class so that the total predicted correctly/total predicted as correct or incorrect (precision) is 50%. When looking at recall and precision, both are the number of predicted for a class but the precision is a ratio of the number of classes it predicted as a class as the denominator while the recall is the true number of classes there are. People have tried shortening it and it leaves out those facts or putting prime over the P as P' to condense the interpretation, but it just adds confusion, and really, needs to be fully written out as such, instead of assuming the readers know the shorthand abbreviations that could have been explained many pages or chapters prior to the current page. I will always bring this up. Because the shorthand differences are the reason for the inconsistencies and misinterpretation or confusion by people who don't use these measures day in and out like their normal cup of coffee. You'll see this in type I (count of false positives) and type II (count of false negatives) errors too for hypothesis testing. And people wired to paraphrase will also get confused there, because they want to condense that to false positives as negatives and false negatives as positives, but really its true negatives labeled positive, and true positives labeled negative. It's not a simple rule as the derivative of a constant is always 0, or the derivative of a x^2 with respect to y is 0. Its a mnemonic of sorts that goes through many versions of shorthand text. I believe you can honestly pull the top 90th percentile of calculus III students aside at random and ask them to calculate the precision and recall and not get consistent results. Because its not used as much in calculus but statistics, and also not seen as really relevant until predictive analytics or machine learning. To them it seems trivial, because class imbalance is irrelevant until trying to improve the accuracy and test out ways to make the class balance produce higher prediction accuracy overall, or separate the classess, and make the model predict within a subset of the classess accurately, but not within the total set of classes. So, lets get to class balancing the best we can.
```{r}
train8b <- training8b %>% group_by(class) %>% count(class)
test8b <- testing8b %>% group_by(class) %>% count(class)
```

```{r}
train8b
```


```{r}
test8b
```

Lets first try training the data on all 86 of the samples except for 1 in each class, then use that 1 sample from each class in the testing set. To select which one to include from each class, lets only keep the one closest to the mean of their class. But we'll use the mean of all the samples in each class.
```{r}

all <- rbind(testing8,training8)
dim(all)
```


```{r}

healthy <- subset(all, all$class=='healthy')
m1 <- subset(all, all$class=='1 month')
m6 <- subset(all, all$class=='6 months')
acute <- subset(all, all$class=='acute')

healthyMean <- order(apply(healthy[,-1],1,mean))
m1Mean <- order(apply(m1[,-1],1,mean))
m6Mean <- order(apply(m6[,-1],1,mean))
acuteMean <- order(apply(acute[,-1],1,mean))
```


```{r}
H1 <- healthyMean[floor(length(healthyMean)/2)]
M1 <- m1Mean[floor(length(m1Mean)/2)]
M6 <- m6Mean[floor(length(m6Mean)/2)]
A1 <- acuteMean[floor(length(acuteMean)/2)]

```


```{r}
testUno <- rbind(healthy[H1,],m1[M1,],m6[M6,],acute[A1,])
testUno
```

```{r}
t1Names <- row.names(testUno)
trainUno <- subset(all, !(row.names(all) %in% t1Names))
dim(trainUno);dim(testUno)
```

Now, lets see how well our model does.
```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfModUno <- train(class~., method='rf', 
               na.action=na.pass,
               data=(trainUno),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRFUno <- predict(rfModUno, testUno)

predDFUno <- data.frame(predRFUno, type=testUno$class)
predDFUno
```


```{r}
pra_uno <- precisionRecallAccuracy(predDFUno)
pra_uno
```

The overall accuracy was improved by taking the samples closer to the mean of each class to use in the test set and train all samples on. But there was one class that was not identified correctly, and another class that was identified correctly but misclassified another class as its own. Maybe we can improve the accuracy even more by removing those class samples that are outside of the standard deviation by the most and then reselecting our one sample per class testing set. Lets see if we can. 

```{r}

healthyStd <- order(apply(healthy[,-1],1,sd))
m1Std <- order(apply(m1[,-1],1,sd))
m6Std <- order(apply(m6[,-1],1,sd))
acuteStd <- order(apply(acute[,-1],1,sd))


```

```{r}
H1b <- healthyStd[length(healthyStd)]
m1b <- m1Std[length(m1Std)]
m6b <- m6Std[length(m6Std)]
acuteb <- acuteStd[length(acuteStd)]
```

```{r}
allb <- all[-c(H1b,m1b,m6b,acuteb),]
```


```{r}
row.names(testUno)==row.names(allb)
```

```{r}
testDos <- testUno
trainDos <- allb
```

Lets see if removing the outliers as far as the sample in each class with the most deviation from the mean of all gene values, will improve accuracy in prediction. 
```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfModDos <- train(class~., method='rf', 
               na.action=na.pass,
               data=(trainDos),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRFDos <- predict(rfModDos, testDos)

predDFDos <- data.frame(predRFDos, type=testDos$class)
predDFDos
```

It worked, we see all classes were predicted correctly by removing the largest standard error sample across all genes, then using the sample closest to the mean of the genes as the samples to predict. We scored 100% accuracy. Lets see this for precision and recall too.
```{r}
pra_dos <- precisionRecallAccuracy(predDFDos)
pra_dos
```

Great! all 100% as it should be since the accuracy was 100%

We could also see what the accuracy is with more samples, like the next closest to the mean, and iteratively after running our model on the growing test sets of samples closest to their class mean, until we can get a measure thats indicative of the population. There must be some discrepencies in the samples as far as what is in their systems, how long they had lyme disease before they began treatment, what vitamins and other medications their on, how old are they, condition of healthy, gender, recent injuries, etc. But otherwise, with all other variables being constant, these genes can identify separate classes of lyme disease or healthy in PBMC or blood. 


***

Lets look at the original data that wasn't de-standardized and see if we can get the same results, better, or filter out the most deviated samples from the average gene expressions. I have been saying this for a while, but more approaches occur in thought before leaving this de-standardized set of gene expression data. 
```{r}
stand <- Lyme9[,-c(88:94)]
standSampleNames <- colnames(stand)[2:87]

month1 <- grep('1month',standSampleNames)
month6 <- grep('6month',standSampleNames)
healthy <- grep('healthy',standSampleNames)
acute <- grep('acute',standSampleNames)

class <- standSampleNames
class[month1] <- '1 month'
class[month6] <- '6 months'
class[healthy] <- 'healthy'
class[acute] <- 'acute'


standGeneNames <- stand$Gene
stand <- as.data.frame(t(stand[,-1]))
colnames(stand) <- standGeneNames
stand$class <- class
stand2 <- stand[,c(34,1:33)]
head(stand2)

```

We could look through all the genes and note the subcategories of behaviors, but there wasn't really an improvement in accuracy when doing this earlier for the de-standardized data. So we will just use all the genes. One gene doesn't have a genes summary, LOC400657 gene, and it won't be in the Tableau charts on this data.But we can still compare how it is in predicting accuracy of classification with our other genes.


Lets split the data into testing and training sets.
```{r}
set.seed(1234)
train2 <- sample(1:86,.7*86)
trainingNorm <- stand2[train2,]
testingNorm <- stand2[-train2,]
dim(trainingNorm);dim(testingNorm)
```



Training/Testing 1:
```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfModNorm <- train(class~., method='rf', 
               na.action=na.pass,
               data=(trainingNorm),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRFNorm <- predict(rfModNorm, testingNorm)

predDFNorm <- data.frame(predRFNorm, type=testingNorm$class)
predDFNorm
```


```{r}
praNorm <- precisionRecallAccuracy(predDFNorm)
praNorm
```

The accuracy was 50% on this log2 normalized data, which is in the same range of accuracy the de-standardized data scored. Lets try removing the samples with the highest standard deviation from the mean of the samples. But first lets see what the class balance or number of samples in each class for each of the training or testing set.The best precision was on the 6 month class, then the 1 month, acute, and healthy classes. The recall was best on the acute class, then the 1 month, 6 month, and healthy class.

```{r}
train2Bal <- trainingNorm %>% group_by(class) %>% count(class)
test2Bal <- testingNorm %>% group_by(class) %>% count(class)
train2Bal
```


```{r}
test2Bal
```

The balance seems to be good, and the 6 to 3 split of the 6 month class seemed to help it score 100% precision accuracy in classification. There must be a lot of variance in those other classes, especially the acute and 1 month, since they had a lot of samples to train and didn't score well when predicting many of those samples.

Lets remove the sample from each class that has the highest deviation and see if it helps in predictions accuracy. 
```{r}
H1nb <- subset(stand2, stand2$class=='healthy')
m1nb <- subset(stand2,stand2$class=='1 month')
m6nb <- subset(stand2, stand2$class=='6 months')
acutenb <- subset(stand2, stand2$class=='acute')
dim(H1nb);dim(m1nb);dim(m6nb);dim(acutenb)
```

The dimensions are as they should be, 21 samples as healthy, 27 samples as acute, 10 samples as 6 months, and 28 samples as 1 month.


```{r}
Hnc <- order(apply(H1nb[,-1],1,sd))
m1nc <- order(apply(m1nb[,-1],1,sd))
m6nc <- order(apply(m6nb[,-1],1,sd))
acnc <- order(apply(acutenb[,-1],1,sd))
```


```{r}
ac_sd1 <- acnc[length(acnc)]
H_sd1 <- Hnc[length(Hnc)]
m1_sd1 <- m1nc[length(m1nc)]
m6_sd1 <- m6nc[length(m6nc)]

stdNormUno <- c(H_sd1,ac_sd1,m1_sd1,m6_sd1)
```


```{r}
stand2_std <- stand2[-stdNormUno,]
dim(stand2);dim(stand2_std)
```

We removed the most deviated samples from each class, now lets split the data and test our classification model we train on it.
```{r}
set.seed(1234)
s <-sample(1:82,.7*82)
trainingNormUno <- stand2_std[s,]
testingNormUno <- stand2_std[-s,]
dim(trainingNormUno);dim(testingNormUno)
```

Lets see the class samples of each set.
```{r}
trainCounts <- trainingNormUno %>% group_by(class) %>% count(class)
trainCounts
```


```{r}
testCounts <- testingNormUno %>% group_by(class) %>% count(class)
testCounts
```

There seems to be a fair distribution of samples in each set. Lets see how our model will classify after removing the most deviated samples of each class.
```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfModNormUno <- train(class~., method='rf', 
               na.action=na.pass,
               data=(trainingNormUno),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRFNormUno <- predict(rfModNormUno, testingNormUno)

predDFNormUno <- data.frame(predRFNormUno, type=testingNormUno$class)
predDFNormUno
```
```{r}
pra_NormUno <- precisionRecallAccuracy(predDFNormUno)
pra_NormUno
```

The accuracy jumped up to 60% from the previous overall accuracy of 50%, so we increased the accuracy 10% better overall by removing a single sample from each class that was the most deviated.The 6 months class scored 100% for accuracy, precision, and recall. So based on these genes we could detect up to 100% accuracy in whether or not a blood sample has had 6 months of antibiotic treatment for lyme disease or not.

Lets not remove any more of the samples from the 6 month class, since we already scored 100% accurate readings. But we should definitely remove some from the other three classes. Lets remove the to 3 classes from those other classes and test the prediction accuracy. 


```{r}
ac_sd1b <- acnc[(length(acnc)-2):length(acnc)]
H_sd1b <- Hnc[(length(Hnc)-2):length(Hnc)]
m1_sd1b <- m1nc[(length(m1nc)-2):length(m1nc)]

m6_sd1b <- m6nc[length(m6nc)]

stdNormUnob <- c(H_sd1b,ac_sd1b,m1_sd1b,m6_sd1b)
```


```{r}
stand2_stdb <- stand2[-stdNormUnob,]
dim(stand2);dim(stand2_stdb)
```

We removed the three most deviated samples from each class except the 6 months class, and now lets split the data and test our classification model we train on it.
```{r}
set.seed(1234)
sb <-sample(1:76,.7*76)
trainingNormUnob <- stand2_stdb[sb,]
testingNormUnob <- stand2_stdb[-sb,]
dim(trainingNormUnob);dim(testingNormUnob)
```

Lets see the class samples of each set.
```{r}
trainCountsb <- trainingNormUnob %>% group_by(class) %>% count(class)
trainCountsb
```


```{r}
testCountsb <- testingNormUnob %>% group_by(class) %>% count(class)
testCountsb
```

There seems to be a fair distribution of samples in each set. Lets see how our model will classify after removing the most deviated samples of each class.
```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfModNormUnob <- train(class~., method='rf', 
               na.action=na.pass,
               data=(trainingNormUnob),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRFNormUnob <- predict(rfModNormUnob, testingNormUnob)

predDFNormUnob <- data.frame(predRFNormUnob, type=testingNormUnob$class)
predDFNormUnob
```

```{r}
pra_NormUnob <- precisionRecallAccuracy(predDFNormUnob)
pra_NormUnob
```

The accuracy of 30% was worse than keeping all samples (50%) and worse than removing the most deviated sample (60%). With one less sample in the 6 months class for the testing set it is not longer 100% but 0% in precision and recall, but still scored 87% accuracy overall for not misclassifying any samples as such. The healthy class also received 0% precision and recall. There were now much less data for the model to train on, so lets change the 70-30 split for training and testing to 95% approximately and 5% testing. Lets see how it does.

```{r}
set.seed(1234)
sc <-sample(1:76,.95*76)
trainingNormUnoc <- stand2_stdb[sc,]
testingNormUnoc <- stand2_stdb[-sc,]
dim(trainingNormUnoc);dim(testingNormUnoc)
```

Lets see the class samples of each set.
```{r}
trainCountsc <- trainingNormUnoc %>% group_by(class) %>% count(class)
trainCountsc
```

```{r}
testCountsc <- testingNormUnoc %>% group_by(class) %>% count(class)
testCountsc
```

One of the 6 months class is missing, so lets take one from the training set and give the training set our extra acute class.

```{r}
dim(trainingNormUnoc)
dim(testingNormUnoc)

s6 <- grep('6month',row.names(trainingNormUnoc))[1]
a6 <- grep('acute',row.names(testingNormUnoc))[2]

S6 <- trainingNormUnoc[s6,]
A6 <- testingNormUnoc[a6,]

testingNormUnoc2 <- testingNormUnoc[-a6,]
testingNormUnoc3 <- rbind(testingNormUnoc2,S6)

trainingNormUnoc2 <- trainingNormUnoc[-s6,]
trainingNormUnoc3 <- rbind(trainingNormUnoc2,A6)

dim(trainingNormUnoc3)
dim(testingNormUnoc3)
```

```{r}
testingNormUnoc3
```

```{r}
tail(trainingNormUnoc3)
```

Now we have at least one of each class in our testing set. This model will train on the data with the three most deviated from the mean samples removed in all classes except the 6 months class, which only has one most deviated sample removed.

```{r, error=FALSE, message=FALSE, warning=FALSE}
set.seed(589647)
rfModNormUnoc3 <- train(class~., method='rf', 
               na.action=na.pass,
               data=(trainingNormUnoc3),  preProc = c("center", "scale","medianImpute"),
               trControl=trainControl(method='oob'), number=5)
```


```{r}
predRFNormUnoc3 <- predict(rfModNormUnoc3, testingNormUnoc3)

predDFNormUnoc3 <- data.frame(predRFNormUnoc3, type=testingNormUnoc3$class)
predDFNormUnoc3
```

```{r}
pra_NormUnoc3 <- precisionRecallAccuracy(predDFNormUnoc3)
pra_NormUnoc3
```

The overall accuracy is 75%, because one class was misclassified as 6 months when it wasn't. That led to a precision of 50% on the 6 month class even though all 6 month classes were predicted accurately (recall of 100%). So this means that originally when keeping all the 6 month samples other than the most deviated we removed, because it scored 100% in prediction for all categories, we now know that removing those additional 6 samples by taking another 2 samples from each of the other three classes affected our model on predicting the 6 months class accurately. But, it did improve the recall and precision for the healthy and acute classes to 100% for precision, recall, and accuracy. but the 1 month class was not identified and was misclassified as the 6 month class. We could be more selective and take only those samples within three standard deviations of the mean and running our algorithms and see how accurate our model is. This is what a course on Linked in learning for recommender systems in python says to do when preprocessing and training your models for classification in that case for sentiment analysis. That course is recommended by Frank Kane, and he has some youtube channel under SunDog he referred to in that online course, but never visited by me. Lets use a little imagination here and say how this effects the model. When someone gives blood at a site, and the values are out of range for the model, then its values in all genes that aren't within the certain selected range of values would throw an error, and the patient would be told his or her sample came out flawed and needs to be done again, with some questionaire on medications taking or not taking a certain vitamin or not drinking water, or drinking more water, or not eating 12-24 hours before, etc. Then if the next time their sample is taken and it is within the range of values, then it can be used to run the model on to predict whether or not the person has lyme disease in the acute phase or if they are healthy. Because this model scored 100% in precision, recall, and accuracy for the healthy and acute classes, but errored on the classes where the patient was taking antibiotics for 1 month and separately 6 months later. So, these genes are also a good set of genes to use as lyme disease pathogenesis, just like our other sample when excluding the most deviated samples from the training model. 





```{r}

```

```{r}

```

```{r}

```




